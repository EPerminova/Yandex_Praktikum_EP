{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#библиотеки\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk \n",
    "import re \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Download Wordnet through NLTK in python console\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_wordnet_posttag(w):\n",
    "    #\"\"\"This function takes in a word which then allocates a tag. Eg: \"fish\" gets a tag v which is VERB\"\"\"\n",
    "    # added in [0][1][0] so as to get main letter. eg VBG, with [0][1][0] gets back V only\n",
    "    tagged = pos_tag([w])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wn.ADJ,\n",
    "        \"N\": wn.NOUN,\n",
    "        \"V\": wn.VERB,\n",
    "        \"R\": wn.ADV\n",
    "    }\n",
    "    return tag_dict.get(tagged, wn.NOUN)  # The OG description is always NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text_clear = re.sub(r'[^a-zA-Z ]', ' ', text) \n",
    "    text_clear = \" \".join(text_clear.split())\n",
    "    word_list = nltk.word_tokenize(text_clear)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list])\n",
    "    return lemmatized_output\n",
    "\n",
    "df['lemm_text'] = df['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>159446</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of ask when your view ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>159447</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that be a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>159448</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer umm there no actual article for prosti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>159449</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it look like it be actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>159450</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and i really don t think you understand i come...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  toxic  \\\n",
       "0                0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1                1  D'aww! He matches this background colour I'm s...      0   \n",
       "2                2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3                3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4                4  You, sir, are my hero. Any chance you remember...      0   \n",
       "...            ...                                                ...    ...   \n",
       "159287      159446  \":::::And for the second time of asking, when ...      0   \n",
       "159288      159447  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159289      159448  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159290      159449  And it looks like it was actually you who put ...      0   \n",
       "159291      159450  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                                lemm_text  \n",
       "0       explanation why the edits make under my userna...  \n",
       "1       d aww he match this background colour i m seem...  \n",
       "2       hey man i m really not try to edit war it s ju...  \n",
       "3       more i can t make any real suggestion on impro...  \n",
       "4       you sir be my hero any chance you remember wha...  \n",
       "...                                                   ...  \n",
       "159287  and for the second time of ask when your view ...  \n",
       "159288  you should be ashamed of yourself that be a ho...  \n",
       "159289  spitzer umm there no actual article for prosti...  \n",
       "159290  and it look like it be actually you who put on...  \n",
       "159291  and i really don t think you understand i come...  \n",
       "\n",
       "[159292 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['lemm_text']\n",
    "target = df['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.15, random_state=12345, stratify = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135398,)\n",
      "(23894,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#стоп слова\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "features_train = count_tf_idf.fit_transform(features_train)\n",
    "features_test = count_tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135398, 137687)\n",
      "(23894, 137687)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135398,)\n",
      "(23894,)\n"
     ]
    }
   ],
   "source": [
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1 способ (пайплайн)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "pipe_lr = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression(random_state=12345))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/2788096778.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             cv=10) \n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "param_range = [9, 10]\n",
    "param_range_fl = [1.0, 0.5]\n",
    "\n",
    "grid_params_lr = [{'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': param_range_fl,\n",
    "        'clf__solver': ['liblinear']}] \n",
    "\n",
    "LR = GridSearchCV(estimator=pipe_lr,\n",
    "            param_grid=grid_params_lr,\n",
    "            scoring='f1',\n",
    "            cv=10) \n",
    "\n",
    "LR.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 1.0, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "print(LR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] START .....................................................................\n",
      "[CV] END ................................ score: (test=0.767) total time=   0.8s\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.761) total time=   0.8s\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    1.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.759) total time=   0.8s\n",
      "f для логической регрессии: 0.7626192352280553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    2.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(solver='liblinear', penalty='l1', C=1, random_state=12345)\n",
    "LR.fit(features_train, target_train)\n",
    "scores_LR_pp = cross_val_score(LR,\n",
    "                            features_train,\n",
    "                            target_train,\n",
    "                            cv=3,\n",
    "                            verbose=10,\n",
    "                            n_jobs=-1,\n",
    "                            scoring='f1') \n",
    "\n",
    "f1_LR_pp = scores_LR_pp.mean()\n",
    "print('f для логической регрессии:', f1_LR_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 способ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3; 1/8] START C=10, penalty=l2, solver=lbfgs..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 1/8] END ............C=10, penalty=l2, solver=lbfgs; total time=  41.0s\n",
      "[CV 2/3; 1/8] START C=10, penalty=l2, solver=lbfgs..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/8] END ............C=10, penalty=l2, solver=lbfgs; total time=  37.5s\n",
      "[CV 3/3; 1/8] START C=10, penalty=l2, solver=lbfgs..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 1/8] END ............C=10, penalty=l2, solver=lbfgs; total time=  38.6s\n",
      "[CV 1/3; 2/8] START C=10, penalty=l2, solver=liblinear..........................\n",
      "[CV 1/3; 2/8] END ........C=10, penalty=l2, solver=liblinear; total time=  13.8s\n",
      "[CV 2/3; 2/8] START C=10, penalty=l2, solver=liblinear..........................\n",
      "[CV 2/3; 2/8] END ........C=10, penalty=l2, solver=liblinear; total time=  11.8s\n",
      "[CV 3/3; 2/8] START C=10, penalty=l2, solver=liblinear..........................\n",
      "[CV 3/3; 2/8] END ........C=10, penalty=l2, solver=liblinear; total time=  11.6s\n",
      "[CV 1/3; 3/8] START C=1.0, penalty=l2, solver=lbfgs.............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 3/8] END ...........C=1.0, penalty=l2, solver=lbfgs; total time=  37.7s\n",
      "[CV 2/3; 3/8] START C=1.0, penalty=l2, solver=lbfgs.............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 3/8] END ...........C=1.0, penalty=l2, solver=lbfgs; total time=  39.3s\n",
      "[CV 3/3; 3/8] START C=1.0, penalty=l2, solver=lbfgs.............................\n",
      "[CV 3/3; 3/8] END ...........C=1.0, penalty=l2, solver=lbfgs; total time=  29.6s\n",
      "[CV 1/3; 4/8] START C=1.0, penalty=l2, solver=liblinear.........................\n",
      "[CV 1/3; 4/8] END .......C=1.0, penalty=l2, solver=liblinear; total time=   6.8s\n",
      "[CV 2/3; 4/8] START C=1.0, penalty=l2, solver=liblinear.........................\n",
      "[CV 2/3; 4/8] END .......C=1.0, penalty=l2, solver=liblinear; total time=   7.7s\n",
      "[CV 3/3; 4/8] START C=1.0, penalty=l2, solver=liblinear.........................\n",
      "[CV 3/3; 4/8] END .......C=1.0, penalty=l2, solver=liblinear; total time=   7.3s\n",
      "[CV 1/3; 5/8] START C=0.1, penalty=l2, solver=lbfgs.............................\n",
      "[CV 1/3; 5/8] END ...........C=0.1, penalty=l2, solver=lbfgs; total time=  15.4s\n",
      "[CV 2/3; 5/8] START C=0.1, penalty=l2, solver=lbfgs.............................\n",
      "[CV 2/3; 5/8] END ...........C=0.1, penalty=l2, solver=lbfgs; total time=  19.6s\n",
      "[CV 3/3; 5/8] START C=0.1, penalty=l2, solver=lbfgs.............................\n",
      "[CV 3/3; 5/8] END ...........C=0.1, penalty=l2, solver=lbfgs; total time=  19.6s\n",
      "[CV 1/3; 6/8] START C=0.1, penalty=l2, solver=liblinear.........................\n",
      "[CV 1/3; 6/8] END .......C=0.1, penalty=l2, solver=liblinear; total time=   4.2s\n",
      "[CV 2/3; 6/8] START C=0.1, penalty=l2, solver=liblinear.........................\n",
      "[CV 2/3; 6/8] END .......C=0.1, penalty=l2, solver=liblinear; total time=   3.5s\n",
      "[CV 3/3; 6/8] START C=0.1, penalty=l2, solver=liblinear.........................\n",
      "[CV 3/3; 6/8] END .......C=0.1, penalty=l2, solver=liblinear; total time=   3.5s\n",
      "[CV 1/3; 7/8] START C=0.01, penalty=l2, solver=lbfgs............................\n",
      "[CV 1/3; 7/8] END ..........C=0.01, penalty=l2, solver=lbfgs; total time=   5.4s\n",
      "[CV 2/3; 7/8] START C=0.01, penalty=l2, solver=lbfgs............................\n",
      "[CV 2/3; 7/8] END ..........C=0.01, penalty=l2, solver=lbfgs; total time=   5.7s\n",
      "[CV 3/3; 7/8] START C=0.01, penalty=l2, solver=lbfgs............................\n",
      "[CV 3/3; 7/8] END ..........C=0.01, penalty=l2, solver=lbfgs; total time=   5.5s\n",
      "[CV 1/3; 8/8] START C=0.01, penalty=l2, solver=liblinear........................\n",
      "[CV 1/3; 8/8] END ......C=0.01, penalty=l2, solver=liblinear; total time=   3.2s\n",
      "[CV 2/3; 8/8] START C=0.01, penalty=l2, solver=liblinear........................\n",
      "[CV 2/3; 8/8] END ......C=0.01, penalty=l2, solver=liblinear; total time=   3.3s\n",
      "[CV 3/3; 8/8] START C=0.01, penalty=l2, solver=liblinear........................\n",
      "[CV 3/3; 8/8] END ......C=0.01, penalty=l2, solver=liblinear; total time=   3.0s\n",
      "CPU times: user 2min 3s, sys: 4min 50s, total: 6min 53s\n",
      "Wall time: 6min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegression(random_state=12345), n_jobs=-1,\n",
       "             param_grid={'C': [10, 1.0, 0.1, 0.01], 'penalty': ['l2'],\n",
       "                         'solver': ['lbfgs', 'liblinear']},\n",
       "             scoring='f1', verbose=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'penalty': ['l2'],\n",
    "    'C': [10, 1.0 ,0.1 ,0.01]}\n",
    "\n",
    "\n",
    "model_LR = LogisticRegression(random_state=12345)\n",
    "\n",
    "model_LR_gs = GridSearchCV(estimator=model_LR, \n",
    "                     param_grid=param_grid,\n",
    "                     cv=3, \n",
    "                     n_jobs=-1, \n",
    "                     verbose=10,\n",
    "                     scoring='f1')\n",
    "model_LR_gs.fit(features_train, target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "print(model_LR_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   41.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.765) total time=  41.6s\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.767) total time=  37.9s\n",
      "[CV] START .....................................................................\n",
      "[CV] END ................................ score: (test=0.754) total time=  39.1s\n",
      "f для логической регрессии: 0.7620445107624816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  2.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  2.0min finished\n"
     ]
    }
   ],
   "source": [
    "model_LR = LogisticRegression(solver='lbfgs', penalty='l2', C=10, random_state=12345)\n",
    "model_LR.fit(features_train, target_train)\n",
    "scores_LR = cross_val_score(model_LR,\n",
    "                            features_train,\n",
    "                            target_train,\n",
    "                            cv=3,\n",
    "                            verbose=10,\n",
    "                            n_jobs=-1,\n",
    "                            scoring='f1') \n",
    "\n",
    "f1_LR = scores_LR.mean()\n",
    "print('f для логической регрессии:', f1_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 171 candidates, totalling 513 fits\n",
      "[CV 1/3; 1/171] START max_depth=1, n_estimators=1...............................\n",
      "[CV 1/3; 1/171] END .............max_depth=1, n_estimators=1; total time=   0.1s\n",
      "[CV 2/3; 1/171] START max_depth=1, n_estimators=1...............................\n",
      "[CV 2/3; 1/171] END .............max_depth=1, n_estimators=1; total time=   0.1s\n",
      "[CV 3/3; 1/171] START max_depth=1, n_estimators=1...............................\n",
      "[CV 3/3; 1/171] END .............max_depth=1, n_estimators=1; total time=   0.1s\n",
      "[CV 1/3; 2/171] START max_depth=1, n_estimators=2...............................\n",
      "[CV 1/3; 2/171] END .............max_depth=1, n_estimators=2; total time=   0.2s\n",
      "[CV 2/3; 2/171] START max_depth=1, n_estimators=2...............................\n",
      "[CV 2/3; 2/171] END .............max_depth=1, n_estimators=2; total time=   0.2s\n",
      "[CV 3/3; 2/171] START max_depth=1, n_estimators=2...............................\n",
      "[CV 3/3; 2/171] END .............max_depth=1, n_estimators=2; total time=   0.2s\n",
      "[CV 1/3; 3/171] START max_depth=1, n_estimators=3...............................\n",
      "[CV 1/3; 3/171] END .............max_depth=1, n_estimators=3; total time=   0.2s\n",
      "[CV 2/3; 3/171] START max_depth=1, n_estimators=3...............................\n",
      "[CV 2/3; 3/171] END .............max_depth=1, n_estimators=3; total time=   0.2s\n",
      "[CV 3/3; 3/171] START max_depth=1, n_estimators=3...............................\n",
      "[CV 3/3; 3/171] END .............max_depth=1, n_estimators=3; total time=   0.2s\n",
      "[CV 1/3; 4/171] START max_depth=1, n_estimators=4...............................\n",
      "[CV 1/3; 4/171] END .............max_depth=1, n_estimators=4; total time=   0.2s\n",
      "[CV 2/3; 4/171] START max_depth=1, n_estimators=4...............................\n",
      "[CV 2/3; 4/171] END .............max_depth=1, n_estimators=4; total time=   0.2s\n",
      "[CV 3/3; 4/171] START max_depth=1, n_estimators=4...............................\n",
      "[CV 3/3; 4/171] END .............max_depth=1, n_estimators=4; total time=   0.2s\n",
      "[CV 1/3; 5/171] START max_depth=1, n_estimators=5...............................\n",
      "[CV 1/3; 5/171] END .............max_depth=1, n_estimators=5; total time=   0.2s\n",
      "[CV 2/3; 5/171] START max_depth=1, n_estimators=5...............................\n",
      "[CV 2/3; 5/171] END .............max_depth=1, n_estimators=5; total time=   0.2s\n",
      "[CV 3/3; 5/171] START max_depth=1, n_estimators=5...............................\n",
      "[CV 3/3; 5/171] END .............max_depth=1, n_estimators=5; total time=   0.2s\n",
      "[CV 1/3; 6/171] START max_depth=1, n_estimators=6...............................\n",
      "[CV 1/3; 6/171] END .............max_depth=1, n_estimators=6; total time=   0.3s\n",
      "[CV 2/3; 6/171] START max_depth=1, n_estimators=6...............................\n",
      "[CV 2/3; 6/171] END .............max_depth=1, n_estimators=6; total time=   0.3s\n",
      "[CV 3/3; 6/171] START max_depth=1, n_estimators=6...............................\n",
      "[CV 3/3; 6/171] END .............max_depth=1, n_estimators=6; total time=   0.3s\n",
      "[CV 1/3; 7/171] START max_depth=1, n_estimators=7...............................\n",
      "[CV 1/3; 7/171] END .............max_depth=1, n_estimators=7; total time=   0.3s\n",
      "[CV 2/3; 7/171] START max_depth=1, n_estimators=7...............................\n",
      "[CV 2/3; 7/171] END .............max_depth=1, n_estimators=7; total time=   0.3s\n",
      "[CV 3/3; 7/171] START max_depth=1, n_estimators=7...............................\n",
      "[CV 3/3; 7/171] END .............max_depth=1, n_estimators=7; total time=   0.3s\n",
      "[CV 1/3; 8/171] START max_depth=1, n_estimators=8...............................\n",
      "[CV 1/3; 8/171] END .............max_depth=1, n_estimators=8; total time=   0.3s\n",
      "[CV 2/3; 8/171] START max_depth=1, n_estimators=8...............................\n",
      "[CV 2/3; 8/171] END .............max_depth=1, n_estimators=8; total time=   0.3s\n",
      "[CV 3/3; 8/171] START max_depth=1, n_estimators=8...............................\n",
      "[CV 3/3; 8/171] END .............max_depth=1, n_estimators=8; total time=   0.3s\n",
      "[CV 1/3; 9/171] START max_depth=1, n_estimators=9...............................\n",
      "[CV 1/3; 9/171] END .............max_depth=1, n_estimators=9; total time=   0.4s\n",
      "[CV 2/3; 9/171] START max_depth=1, n_estimators=9...............................\n",
      "[CV 2/3; 9/171] END .............max_depth=1, n_estimators=9; total time=   0.4s\n",
      "[CV 3/3; 9/171] START max_depth=1, n_estimators=9...............................\n",
      "[CV 3/3; 9/171] END .............max_depth=1, n_estimators=9; total time=   0.3s\n",
      "[CV 1/3; 10/171] START max_depth=1, n_estimators=10.............................\n",
      "[CV 1/3; 10/171] END ...........max_depth=1, n_estimators=10; total time=   0.4s\n",
      "[CV 2/3; 10/171] START max_depth=1, n_estimators=10.............................\n",
      "[CV 2/3; 10/171] END ...........max_depth=1, n_estimators=10; total time=   0.4s\n",
      "[CV 3/3; 10/171] START max_depth=1, n_estimators=10.............................\n",
      "[CV 3/3; 10/171] END ...........max_depth=1, n_estimators=10; total time=   0.4s\n",
      "[CV 1/3; 11/171] START max_depth=1, n_estimators=11.............................\n",
      "[CV 1/3; 11/171] END ...........max_depth=1, n_estimators=11; total time=   0.4s\n",
      "[CV 2/3; 11/171] START max_depth=1, n_estimators=11.............................\n",
      "[CV 2/3; 11/171] END ...........max_depth=1, n_estimators=11; total time=   0.4s\n",
      "[CV 3/3; 11/171] START max_depth=1, n_estimators=11.............................\n",
      "[CV 3/3; 11/171] END ...........max_depth=1, n_estimators=11; total time=   0.4s\n",
      "[CV 1/3; 12/171] START max_depth=1, n_estimators=12.............................\n",
      "[CV 1/3; 12/171] END ...........max_depth=1, n_estimators=12; total time=   0.4s\n",
      "[CV 2/3; 12/171] START max_depth=1, n_estimators=12.............................\n",
      "[CV 2/3; 12/171] END ...........max_depth=1, n_estimators=12; total time=   0.4s\n",
      "[CV 3/3; 12/171] START max_depth=1, n_estimators=12.............................\n",
      "[CV 3/3; 12/171] END ...........max_depth=1, n_estimators=12; total time=   0.5s\n",
      "[CV 1/3; 13/171] START max_depth=1, n_estimators=13.............................\n",
      "[CV 1/3; 13/171] END ...........max_depth=1, n_estimators=13; total time=   0.4s\n",
      "[CV 2/3; 13/171] START max_depth=1, n_estimators=13.............................\n",
      "[CV 2/3; 13/171] END ...........max_depth=1, n_estimators=13; total time=   0.5s\n",
      "[CV 3/3; 13/171] START max_depth=1, n_estimators=13.............................\n",
      "[CV 3/3; 13/171] END ...........max_depth=1, n_estimators=13; total time=   0.5s\n",
      "[CV 1/3; 14/171] START max_depth=1, n_estimators=14.............................\n",
      "[CV 1/3; 14/171] END ...........max_depth=1, n_estimators=14; total time=   0.5s\n",
      "[CV 2/3; 14/171] START max_depth=1, n_estimators=14.............................\n",
      "[CV 2/3; 14/171] END ...........max_depth=1, n_estimators=14; total time=   0.5s\n",
      "[CV 3/3; 14/171] START max_depth=1, n_estimators=14.............................\n",
      "[CV 3/3; 14/171] END ...........max_depth=1, n_estimators=14; total time=   0.5s\n",
      "[CV 1/3; 15/171] START max_depth=1, n_estimators=15.............................\n",
      "[CV 1/3; 15/171] END ...........max_depth=1, n_estimators=15; total time=   0.5s\n",
      "[CV 2/3; 15/171] START max_depth=1, n_estimators=15.............................\n",
      "[CV 2/3; 15/171] END ...........max_depth=1, n_estimators=15; total time=   0.5s\n",
      "[CV 3/3; 15/171] START max_depth=1, n_estimators=15.............................\n",
      "[CV 3/3; 15/171] END ...........max_depth=1, n_estimators=15; total time=   0.5s\n",
      "[CV 1/3; 16/171] START max_depth=1, n_estimators=16.............................\n",
      "[CV 1/3; 16/171] END ...........max_depth=1, n_estimators=16; total time=   0.5s\n",
      "[CV 2/3; 16/171] START max_depth=1, n_estimators=16.............................\n",
      "[CV 2/3; 16/171] END ...........max_depth=1, n_estimators=16; total time=   0.5s\n",
      "[CV 3/3; 16/171] START max_depth=1, n_estimators=16.............................\n",
      "[CV 3/3; 16/171] END ...........max_depth=1, n_estimators=16; total time=   0.5s\n",
      "[CV 1/3; 17/171] START max_depth=1, n_estimators=17.............................\n",
      "[CV 1/3; 17/171] END ...........max_depth=1, n_estimators=17; total time=   0.5s\n",
      "[CV 2/3; 17/171] START max_depth=1, n_estimators=17.............................\n",
      "[CV 2/3; 17/171] END ...........max_depth=1, n_estimators=17; total time=   0.6s\n",
      "[CV 3/3; 17/171] START max_depth=1, n_estimators=17.............................\n",
      "[CV 3/3; 17/171] END ...........max_depth=1, n_estimators=17; total time=   0.5s\n",
      "[CV 1/3; 18/171] START max_depth=1, n_estimators=18.............................\n",
      "[CV 1/3; 18/171] END ...........max_depth=1, n_estimators=18; total time=   0.6s\n",
      "[CV 2/3; 18/171] START max_depth=1, n_estimators=18.............................\n",
      "[CV 2/3; 18/171] END ...........max_depth=1, n_estimators=18; total time=   0.6s\n",
      "[CV 3/3; 18/171] START max_depth=1, n_estimators=18.............................\n",
      "[CV 3/3; 18/171] END ...........max_depth=1, n_estimators=18; total time=   0.6s\n",
      "[CV 1/3; 19/171] START max_depth=1, n_estimators=19.............................\n",
      "[CV 1/3; 19/171] END ...........max_depth=1, n_estimators=19; total time=   0.6s\n",
      "[CV 2/3; 19/171] START max_depth=1, n_estimators=19.............................\n",
      "[CV 2/3; 19/171] END ...........max_depth=1, n_estimators=19; total time=   0.6s\n",
      "[CV 3/3; 19/171] START max_depth=1, n_estimators=19.............................\n",
      "[CV 3/3; 19/171] END ...........max_depth=1, n_estimators=19; total time=   0.6s\n",
      "[CV 1/3; 20/171] START max_depth=2, n_estimators=1..............................\n",
      "[CV 1/3; 20/171] END ............max_depth=2, n_estimators=1; total time=   0.1s\n",
      "[CV 2/3; 20/171] START max_depth=2, n_estimators=1..............................\n",
      "[CV 2/3; 20/171] END ............max_depth=2, n_estimators=1; total time=   0.1s\n",
      "[CV 3/3; 20/171] START max_depth=2, n_estimators=1..............................\n",
      "[CV 3/3; 20/171] END ............max_depth=2, n_estimators=1; total time=   0.1s\n",
      "[CV 1/3; 21/171] START max_depth=2, n_estimators=2..............................\n",
      "[CV 1/3; 21/171] END ............max_depth=2, n_estimators=2; total time=   0.2s\n",
      "[CV 2/3; 21/171] START max_depth=2, n_estimators=2..............................\n",
      "[CV 2/3; 21/171] END ............max_depth=2, n_estimators=2; total time=   0.2s\n",
      "[CV 3/3; 21/171] START max_depth=2, n_estimators=2..............................\n",
      "[CV 3/3; 21/171] END ............max_depth=2, n_estimators=2; total time=   0.2s\n",
      "[CV 1/3; 22/171] START max_depth=2, n_estimators=3..............................\n",
      "[CV 1/3; 22/171] END ............max_depth=2, n_estimators=3; total time=   0.2s\n",
      "[CV 2/3; 22/171] START max_depth=2, n_estimators=3..............................\n",
      "[CV 2/3; 22/171] END ............max_depth=2, n_estimators=3; total time=   0.2s\n",
      "[CV 3/3; 22/171] START max_depth=2, n_estimators=3..............................\n",
      "[CV 3/3; 22/171] END ............max_depth=2, n_estimators=3; total time=   0.2s\n",
      "[CV 1/3; 23/171] START max_depth=2, n_estimators=4..............................\n",
      "[CV 1/3; 23/171] END ............max_depth=2, n_estimators=4; total time=   0.3s\n",
      "[CV 2/3; 23/171] START max_depth=2, n_estimators=4..............................\n",
      "[CV 2/3; 23/171] END ............max_depth=2, n_estimators=4; total time=   0.3s\n",
      "[CV 3/3; 23/171] START max_depth=2, n_estimators=4..............................\n",
      "[CV 3/3; 23/171] END ............max_depth=2, n_estimators=4; total time=   0.3s\n",
      "[CV 1/3; 24/171] START max_depth=2, n_estimators=5..............................\n",
      "[CV 1/3; 24/171] END ............max_depth=2, n_estimators=5; total time=   0.3s\n",
      "[CV 2/3; 24/171] START max_depth=2, n_estimators=5..............................\n",
      "[CV 2/3; 24/171] END ............max_depth=2, n_estimators=5; total time=   0.3s\n",
      "[CV 3/3; 24/171] START max_depth=2, n_estimators=5..............................\n",
      "[CV 3/3; 24/171] END ............max_depth=2, n_estimators=5; total time=   0.3s\n",
      "[CV 1/3; 25/171] START max_depth=2, n_estimators=6..............................\n",
      "[CV 1/3; 25/171] END ............max_depth=2, n_estimators=6; total time=   0.4s\n",
      "[CV 2/3; 25/171] START max_depth=2, n_estimators=6..............................\n",
      "[CV 2/3; 25/171] END ............max_depth=2, n_estimators=6; total time=   0.4s\n",
      "[CV 3/3; 25/171] START max_depth=2, n_estimators=6..............................\n",
      "[CV 3/3; 25/171] END ............max_depth=2, n_estimators=6; total time=   0.4s\n",
      "[CV 1/3; 26/171] START max_depth=2, n_estimators=7..............................\n",
      "[CV 1/3; 26/171] END ............max_depth=2, n_estimators=7; total time=   0.4s\n",
      "[CV 2/3; 26/171] START max_depth=2, n_estimators=7..............................\n",
      "[CV 2/3; 26/171] END ............max_depth=2, n_estimators=7; total time=   0.4s\n",
      "[CV 3/3; 26/171] START max_depth=2, n_estimators=7..............................\n",
      "[CV 3/3; 26/171] END ............max_depth=2, n_estimators=7; total time=   0.4s\n",
      "[CV 1/3; 27/171] START max_depth=2, n_estimators=8..............................\n",
      "[CV 1/3; 27/171] END ............max_depth=2, n_estimators=8; total time=   0.4s\n",
      "[CV 2/3; 27/171] START max_depth=2, n_estimators=8..............................\n",
      "[CV 2/3; 27/171] END ............max_depth=2, n_estimators=8; total time=   0.4s\n",
      "[CV 3/3; 27/171] START max_depth=2, n_estimators=8..............................\n",
      "[CV 3/3; 27/171] END ............max_depth=2, n_estimators=8; total time=   0.4s\n",
      "[CV 1/3; 28/171] START max_depth=2, n_estimators=9..............................\n",
      "[CV 1/3; 28/171] END ............max_depth=2, n_estimators=9; total time=   0.5s\n",
      "[CV 2/3; 28/171] START max_depth=2, n_estimators=9..............................\n",
      "[CV 2/3; 28/171] END ............max_depth=2, n_estimators=9; total time=   0.5s\n",
      "[CV 3/3; 28/171] START max_depth=2, n_estimators=9..............................\n",
      "[CV 3/3; 28/171] END ............max_depth=2, n_estimators=9; total time=   0.5s\n",
      "[CV 1/3; 29/171] START max_depth=2, n_estimators=10.............................\n",
      "[CV 1/3; 29/171] END ...........max_depth=2, n_estimators=10; total time=   0.5s\n",
      "[CV 2/3; 29/171] START max_depth=2, n_estimators=10.............................\n",
      "[CV 2/3; 29/171] END ...........max_depth=2, n_estimators=10; total time=   0.5s\n",
      "[CV 3/3; 29/171] START max_depth=2, n_estimators=10.............................\n",
      "[CV 3/3; 29/171] END ...........max_depth=2, n_estimators=10; total time=   0.5s\n",
      "[CV 1/3; 30/171] START max_depth=2, n_estimators=11.............................\n",
      "[CV 1/3; 30/171] END ...........max_depth=2, n_estimators=11; total time=   0.5s\n",
      "[CV 2/3; 30/171] START max_depth=2, n_estimators=11.............................\n",
      "[CV 2/3; 30/171] END ...........max_depth=2, n_estimators=11; total time=   0.5s\n",
      "[CV 3/3; 30/171] START max_depth=2, n_estimators=11.............................\n",
      "[CV 3/3; 30/171] END ...........max_depth=2, n_estimators=11; total time=   0.5s\n",
      "[CV 1/3; 31/171] START max_depth=2, n_estimators=12.............................\n",
      "[CV 1/3; 31/171] END ...........max_depth=2, n_estimators=12; total time=   0.6s\n",
      "[CV 2/3; 31/171] START max_depth=2, n_estimators=12.............................\n",
      "[CV 2/3; 31/171] END ...........max_depth=2, n_estimators=12; total time=   0.6s\n",
      "[CV 3/3; 31/171] START max_depth=2, n_estimators=12.............................\n",
      "[CV 3/3; 31/171] END ...........max_depth=2, n_estimators=12; total time=   0.6s\n",
      "[CV 1/3; 32/171] START max_depth=2, n_estimators=13.............................\n",
      "[CV 1/3; 32/171] END ...........max_depth=2, n_estimators=13; total time=   0.6s\n",
      "[CV 2/3; 32/171] START max_depth=2, n_estimators=13.............................\n",
      "[CV 2/3; 32/171] END ...........max_depth=2, n_estimators=13; total time=   0.6s\n",
      "[CV 3/3; 32/171] START max_depth=2, n_estimators=13.............................\n",
      "[CV 3/3; 32/171] END ...........max_depth=2, n_estimators=13; total time=   0.6s\n",
      "[CV 1/3; 33/171] START max_depth=2, n_estimators=14.............................\n",
      "[CV 1/3; 33/171] END ...........max_depth=2, n_estimators=14; total time=   0.6s\n",
      "[CV 2/3; 33/171] START max_depth=2, n_estimators=14.............................\n",
      "[CV 2/3; 33/171] END ...........max_depth=2, n_estimators=14; total time=   0.6s\n",
      "[CV 3/3; 33/171] START max_depth=2, n_estimators=14.............................\n",
      "[CV 3/3; 33/171] END ...........max_depth=2, n_estimators=14; total time=   0.6s\n",
      "[CV 1/3; 34/171] START max_depth=2, n_estimators=15.............................\n",
      "[CV 1/3; 34/171] END ...........max_depth=2, n_estimators=15; total time=   0.7s\n",
      "[CV 2/3; 34/171] START max_depth=2, n_estimators=15.............................\n",
      "[CV 2/3; 34/171] END ...........max_depth=2, n_estimators=15; total time=   0.7s\n",
      "[CV 3/3; 34/171] START max_depth=2, n_estimators=15.............................\n",
      "[CV 3/3; 34/171] END ...........max_depth=2, n_estimators=15; total time=   0.7s\n",
      "[CV 1/3; 35/171] START max_depth=2, n_estimators=16.............................\n",
      "[CV 1/3; 35/171] END ...........max_depth=2, n_estimators=16; total time=   0.7s\n",
      "[CV 2/3; 35/171] START max_depth=2, n_estimators=16.............................\n",
      "[CV 2/3; 35/171] END ...........max_depth=2, n_estimators=16; total time=   0.7s\n",
      "[CV 3/3; 35/171] START max_depth=2, n_estimators=16.............................\n",
      "[CV 3/3; 35/171] END ...........max_depth=2, n_estimators=16; total time=   0.7s\n",
      "[CV 1/3; 36/171] START max_depth=2, n_estimators=17.............................\n",
      "[CV 1/3; 36/171] END ...........max_depth=2, n_estimators=17; total time=   0.8s\n",
      "[CV 2/3; 36/171] START max_depth=2, n_estimators=17.............................\n",
      "[CV 2/3; 36/171] END ...........max_depth=2, n_estimators=17; total time=   0.8s\n",
      "[CV 3/3; 36/171] START max_depth=2, n_estimators=17.............................\n",
      "[CV 3/3; 36/171] END ...........max_depth=2, n_estimators=17; total time=   0.8s\n",
      "[CV 1/3; 37/171] START max_depth=2, n_estimators=18.............................\n",
      "[CV 1/3; 37/171] END ...........max_depth=2, n_estimators=18; total time=   0.9s\n",
      "[CV 2/3; 37/171] START max_depth=2, n_estimators=18.............................\n",
      "[CV 2/3; 37/171] END ...........max_depth=2, n_estimators=18; total time=   0.8s\n",
      "[CV 3/3; 37/171] START max_depth=2, n_estimators=18.............................\n",
      "[CV 3/3; 37/171] END ...........max_depth=2, n_estimators=18; total time=   0.8s\n",
      "[CV 1/3; 38/171] START max_depth=2, n_estimators=19.............................\n",
      "[CV 1/3; 38/171] END ...........max_depth=2, n_estimators=19; total time=   0.8s\n",
      "[CV 2/3; 38/171] START max_depth=2, n_estimators=19.............................\n",
      "[CV 2/3; 38/171] END ...........max_depth=2, n_estimators=19; total time=   0.9s\n",
      "[CV 3/3; 38/171] START max_depth=2, n_estimators=19.............................\n",
      "[CV 3/3; 38/171] END ...........max_depth=2, n_estimators=19; total time=   0.8s\n",
      "[CV 1/3; 39/171] START max_depth=3, n_estimators=1..............................\n",
      "[CV 1/3; 39/171] END ............max_depth=3, n_estimators=1; total time=   0.2s\n",
      "[CV 2/3; 39/171] START max_depth=3, n_estimators=1..............................\n",
      "[CV 2/3; 39/171] END ............max_depth=3, n_estimators=1; total time=   0.2s\n",
      "[CV 3/3; 39/171] START max_depth=3, n_estimators=1..............................\n",
      "[CV 3/3; 39/171] END ............max_depth=3, n_estimators=1; total time=   0.2s\n",
      "[CV 1/3; 40/171] START max_depth=3, n_estimators=2..............................\n",
      "[CV 1/3; 40/171] END ............max_depth=3, n_estimators=2; total time=   0.2s\n",
      "[CV 2/3; 40/171] START max_depth=3, n_estimators=2..............................\n",
      "[CV 2/3; 40/171] END ............max_depth=3, n_estimators=2; total time=   0.2s\n",
      "[CV 3/3; 40/171] START max_depth=3, n_estimators=2..............................\n",
      "[CV 3/3; 40/171] END ............max_depth=3, n_estimators=2; total time=   0.2s\n",
      "[CV 1/3; 41/171] START max_depth=3, n_estimators=3..............................\n",
      "[CV 1/3; 41/171] END ............max_depth=3, n_estimators=3; total time=   0.3s\n",
      "[CV 2/3; 41/171] START max_depth=3, n_estimators=3..............................\n",
      "[CV 2/3; 41/171] END ............max_depth=3, n_estimators=3; total time=   0.3s\n",
      "[CV 3/3; 41/171] START max_depth=3, n_estimators=3..............................\n",
      "[CV 3/3; 41/171] END ............max_depth=3, n_estimators=3; total time=   0.3s\n",
      "[CV 1/3; 42/171] START max_depth=3, n_estimators=4..............................\n",
      "[CV 1/3; 42/171] END ............max_depth=3, n_estimators=4; total time=   0.3s\n",
      "[CV 2/3; 42/171] START max_depth=3, n_estimators=4..............................\n",
      "[CV 2/3; 42/171] END ............max_depth=3, n_estimators=4; total time=   0.3s\n",
      "[CV 3/3; 42/171] START max_depth=3, n_estimators=4..............................\n",
      "[CV 3/3; 42/171] END ............max_depth=3, n_estimators=4; total time=   0.3s\n",
      "[CV 1/3; 43/171] START max_depth=3, n_estimators=5..............................\n",
      "[CV 1/3; 43/171] END ............max_depth=3, n_estimators=5; total time=   0.4s\n",
      "[CV 2/3; 43/171] START max_depth=3, n_estimators=5..............................\n",
      "[CV 2/3; 43/171] END ............max_depth=3, n_estimators=5; total time=   0.4s\n",
      "[CV 3/3; 43/171] START max_depth=3, n_estimators=5..............................\n",
      "[CV 3/3; 43/171] END ............max_depth=3, n_estimators=5; total time=   0.4s\n",
      "[CV 1/3; 44/171] START max_depth=3, n_estimators=6..............................\n",
      "[CV 1/3; 44/171] END ............max_depth=3, n_estimators=6; total time=   0.4s\n",
      "[CV 2/3; 44/171] START max_depth=3, n_estimators=6..............................\n",
      "[CV 2/3; 44/171] END ............max_depth=3, n_estimators=6; total time=   0.4s\n",
      "[CV 3/3; 44/171] START max_depth=3, n_estimators=6..............................\n",
      "[CV 3/3; 44/171] END ............max_depth=3, n_estimators=6; total time=   0.4s\n",
      "[CV 1/3; 45/171] START max_depth=3, n_estimators=7..............................\n",
      "[CV 1/3; 45/171] END ............max_depth=3, n_estimators=7; total time=   0.5s\n",
      "[CV 2/3; 45/171] START max_depth=3, n_estimators=7..............................\n",
      "[CV 2/3; 45/171] END ............max_depth=3, n_estimators=7; total time=   0.5s\n",
      "[CV 3/3; 45/171] START max_depth=3, n_estimators=7..............................\n",
      "[CV 3/3; 45/171] END ............max_depth=3, n_estimators=7; total time=   0.5s\n",
      "[CV 1/3; 46/171] START max_depth=3, n_estimators=8..............................\n",
      "[CV 1/3; 46/171] END ............max_depth=3, n_estimators=8; total time=   0.5s\n",
      "[CV 2/3; 46/171] START max_depth=3, n_estimators=8..............................\n",
      "[CV 2/3; 46/171] END ............max_depth=3, n_estimators=8; total time=   0.5s\n",
      "[CV 3/3; 46/171] START max_depth=3, n_estimators=8..............................\n",
      "[CV 3/3; 46/171] END ............max_depth=3, n_estimators=8; total time=   0.5s\n",
      "[CV 1/3; 47/171] START max_depth=3, n_estimators=9..............................\n",
      "[CV 1/3; 47/171] END ............max_depth=3, n_estimators=9; total time=   0.6s\n",
      "[CV 2/3; 47/171] START max_depth=3, n_estimators=9..............................\n",
      "[CV 2/3; 47/171] END ............max_depth=3, n_estimators=9; total time=   0.6s\n",
      "[CV 3/3; 47/171] START max_depth=3, n_estimators=9..............................\n",
      "[CV 3/3; 47/171] END ............max_depth=3, n_estimators=9; total time=   0.6s\n",
      "[CV 1/3; 48/171] START max_depth=3, n_estimators=10.............................\n",
      "[CV 1/3; 48/171] END ...........max_depth=3, n_estimators=10; total time=   0.7s\n",
      "[CV 2/3; 48/171] START max_depth=3, n_estimators=10.............................\n",
      "[CV 2/3; 48/171] END ...........max_depth=3, n_estimators=10; total time=   0.6s\n",
      "[CV 3/3; 48/171] START max_depth=3, n_estimators=10.............................\n",
      "[CV 3/3; 48/171] END ...........max_depth=3, n_estimators=10; total time=   0.6s\n",
      "[CV 1/3; 49/171] START max_depth=3, n_estimators=11.............................\n",
      "[CV 1/3; 49/171] END ...........max_depth=3, n_estimators=11; total time=   0.7s\n",
      "[CV 2/3; 49/171] START max_depth=3, n_estimators=11.............................\n",
      "[CV 2/3; 49/171] END ...........max_depth=3, n_estimators=11; total time=   0.7s\n",
      "[CV 3/3; 49/171] START max_depth=3, n_estimators=11.............................\n",
      "[CV 3/3; 49/171] END ...........max_depth=3, n_estimators=11; total time=   0.8s\n",
      "[CV 1/3; 50/171] START max_depth=3, n_estimators=12.............................\n",
      "[CV 1/3; 50/171] END ...........max_depth=3, n_estimators=12; total time=   0.8s\n",
      "[CV 2/3; 50/171] START max_depth=3, n_estimators=12.............................\n",
      "[CV 2/3; 50/171] END ...........max_depth=3, n_estimators=12; total time=   0.8s\n",
      "[CV 3/3; 50/171] START max_depth=3, n_estimators=12.............................\n",
      "[CV 3/3; 50/171] END ...........max_depth=3, n_estimators=12; total time=   0.8s\n",
      "[CV 1/3; 51/171] START max_depth=3, n_estimators=13.............................\n",
      "[CV 1/3; 51/171] END ...........max_depth=3, n_estimators=13; total time=   0.8s\n",
      "[CV 2/3; 51/171] START max_depth=3, n_estimators=13.............................\n",
      "[CV 2/3; 51/171] END ...........max_depth=3, n_estimators=13; total time=   0.8s\n",
      "[CV 3/3; 51/171] START max_depth=3, n_estimators=13.............................\n",
      "[CV 3/3; 51/171] END ...........max_depth=3, n_estimators=13; total time=   0.9s\n",
      "[CV 1/3; 52/171] START max_depth=3, n_estimators=14.............................\n",
      "[CV 1/3; 52/171] END ...........max_depth=3, n_estimators=14; total time=   0.8s\n",
      "[CV 2/3; 52/171] START max_depth=3, n_estimators=14.............................\n",
      "[CV 2/3; 52/171] END ...........max_depth=3, n_estimators=14; total time=   0.8s\n",
      "[CV 3/3; 52/171] START max_depth=3, n_estimators=14.............................\n",
      "[CV 3/3; 52/171] END ...........max_depth=3, n_estimators=14; total time=   0.8s\n",
      "[CV 1/3; 53/171] START max_depth=3, n_estimators=15.............................\n",
      "[CV 1/3; 53/171] END ...........max_depth=3, n_estimators=15; total time=   0.9s\n",
      "[CV 2/3; 53/171] START max_depth=3, n_estimators=15.............................\n",
      "[CV 2/3; 53/171] END ...........max_depth=3, n_estimators=15; total time=   0.9s\n",
      "[CV 3/3; 53/171] START max_depth=3, n_estimators=15.............................\n",
      "[CV 3/3; 53/171] END ...........max_depth=3, n_estimators=15; total time=   0.9s\n",
      "[CV 1/3; 54/171] START max_depth=3, n_estimators=16.............................\n",
      "[CV 1/3; 54/171] END ...........max_depth=3, n_estimators=16; total time=   1.0s\n",
      "[CV 2/3; 54/171] START max_depth=3, n_estimators=16.............................\n",
      "[CV 2/3; 54/171] END ...........max_depth=3, n_estimators=16; total time=   1.0s\n",
      "[CV 3/3; 54/171] START max_depth=3, n_estimators=16.............................\n",
      "[CV 3/3; 54/171] END ...........max_depth=3, n_estimators=16; total time=   1.0s\n",
      "[CV 1/3; 55/171] START max_depth=3, n_estimators=17.............................\n",
      "[CV 1/3; 55/171] END ...........max_depth=3, n_estimators=17; total time=   1.0s\n",
      "[CV 2/3; 55/171] START max_depth=3, n_estimators=17.............................\n",
      "[CV 2/3; 55/171] END ...........max_depth=3, n_estimators=17; total time=   1.1s\n",
      "[CV 3/3; 55/171] START max_depth=3, n_estimators=17.............................\n",
      "[CV 3/3; 55/171] END ...........max_depth=3, n_estimators=17; total time=   1.0s\n",
      "[CV 1/3; 56/171] START max_depth=3, n_estimators=18.............................\n",
      "[CV 1/3; 56/171] END ...........max_depth=3, n_estimators=18; total time=   1.1s\n",
      "[CV 2/3; 56/171] START max_depth=3, n_estimators=18.............................\n",
      "[CV 2/3; 56/171] END ...........max_depth=3, n_estimators=18; total time=   1.1s\n",
      "[CV 3/3; 56/171] START max_depth=3, n_estimators=18.............................\n",
      "[CV 3/3; 56/171] END ...........max_depth=3, n_estimators=18; total time=   1.1s\n",
      "[CV 1/3; 57/171] START max_depth=3, n_estimators=19.............................\n",
      "[CV 1/3; 57/171] END ...........max_depth=3, n_estimators=19; total time=   1.1s\n",
      "[CV 2/3; 57/171] START max_depth=3, n_estimators=19.............................\n",
      "[CV 2/3; 57/171] END ...........max_depth=3, n_estimators=19; total time=   1.1s\n",
      "[CV 3/3; 57/171] START max_depth=3, n_estimators=19.............................\n",
      "[CV 3/3; 57/171] END ...........max_depth=3, n_estimators=19; total time=   1.1s\n",
      "[CV 1/3; 58/171] START max_depth=4, n_estimators=1..............................\n",
      "[CV 1/3; 58/171] END ............max_depth=4, n_estimators=1; total time=   0.2s\n",
      "[CV 2/3; 58/171] START max_depth=4, n_estimators=1..............................\n",
      "[CV 2/3; 58/171] END ............max_depth=4, n_estimators=1; total time=   0.2s\n",
      "[CV 3/3; 58/171] START max_depth=4, n_estimators=1..............................\n",
      "[CV 3/3; 58/171] END ............max_depth=4, n_estimators=1; total time=   0.2s\n",
      "[CV 1/3; 59/171] START max_depth=4, n_estimators=2..............................\n",
      "[CV 1/3; 59/171] END ............max_depth=4, n_estimators=2; total time=   0.2s\n",
      "[CV 2/3; 59/171] START max_depth=4, n_estimators=2..............................\n",
      "[CV 2/3; 59/171] END ............max_depth=4, n_estimators=2; total time=   0.2s\n",
      "[CV 3/3; 59/171] START max_depth=4, n_estimators=2..............................\n",
      "[CV 3/3; 59/171] END ............max_depth=4, n_estimators=2; total time=   0.2s\n",
      "[CV 1/3; 60/171] START max_depth=4, n_estimators=3..............................\n",
      "[CV 1/3; 60/171] END ............max_depth=4, n_estimators=3; total time=   0.3s\n",
      "[CV 2/3; 60/171] START max_depth=4, n_estimators=3..............................\n",
      "[CV 2/3; 60/171] END ............max_depth=4, n_estimators=3; total time=   0.3s\n",
      "[CV 3/3; 60/171] START max_depth=4, n_estimators=3..............................\n",
      "[CV 3/3; 60/171] END ............max_depth=4, n_estimators=3; total time=   0.3s\n",
      "[CV 1/3; 61/171] START max_depth=4, n_estimators=4..............................\n",
      "[CV 1/3; 61/171] END ............max_depth=4, n_estimators=4; total time=   0.4s\n",
      "[CV 2/3; 61/171] START max_depth=4, n_estimators=4..............................\n",
      "[CV 2/3; 61/171] END ............max_depth=4, n_estimators=4; total time=   0.4s\n",
      "[CV 3/3; 61/171] START max_depth=4, n_estimators=4..............................\n",
      "[CV 3/3; 61/171] END ............max_depth=4, n_estimators=4; total time=   0.4s\n",
      "[CV 1/3; 62/171] START max_depth=4, n_estimators=5..............................\n",
      "[CV 1/3; 62/171] END ............max_depth=4, n_estimators=5; total time=   0.4s\n",
      "[CV 2/3; 62/171] START max_depth=4, n_estimators=5..............................\n",
      "[CV 2/3; 62/171] END ............max_depth=4, n_estimators=5; total time=   0.4s\n",
      "[CV 3/3; 62/171] START max_depth=4, n_estimators=5..............................\n",
      "[CV 3/3; 62/171] END ............max_depth=4, n_estimators=5; total time=   0.5s\n",
      "[CV 1/3; 63/171] START max_depth=4, n_estimators=6..............................\n",
      "[CV 1/3; 63/171] END ............max_depth=4, n_estimators=6; total time=   0.5s\n",
      "[CV 2/3; 63/171] START max_depth=4, n_estimators=6..............................\n",
      "[CV 2/3; 63/171] END ............max_depth=4, n_estimators=6; total time=   0.5s\n",
      "[CV 3/3; 63/171] START max_depth=4, n_estimators=6..............................\n",
      "[CV 3/3; 63/171] END ............max_depth=4, n_estimators=6; total time=   0.5s\n",
      "[CV 1/3; 64/171] START max_depth=4, n_estimators=7..............................\n",
      "[CV 1/3; 64/171] END ............max_depth=4, n_estimators=7; total time=   0.6s\n",
      "[CV 2/3; 64/171] START max_depth=4, n_estimators=7..............................\n",
      "[CV 2/3; 64/171] END ............max_depth=4, n_estimators=7; total time=   0.6s\n",
      "[CV 3/3; 64/171] START max_depth=4, n_estimators=7..............................\n",
      "[CV 3/3; 64/171] END ............max_depth=4, n_estimators=7; total time=   0.6s\n",
      "[CV 1/3; 65/171] START max_depth=4, n_estimators=8..............................\n",
      "[CV 1/3; 65/171] END ............max_depth=4, n_estimators=8; total time=   0.7s\n",
      "[CV 2/3; 65/171] START max_depth=4, n_estimators=8..............................\n",
      "[CV 2/3; 65/171] END ............max_depth=4, n_estimators=8; total time=   0.7s\n",
      "[CV 3/3; 65/171] START max_depth=4, n_estimators=8..............................\n",
      "[CV 3/3; 65/171] END ............max_depth=4, n_estimators=8; total time=   0.7s\n",
      "[CV 1/3; 66/171] START max_depth=4, n_estimators=9..............................\n",
      "[CV 1/3; 66/171] END ............max_depth=4, n_estimators=9; total time=   0.7s\n",
      "[CV 2/3; 66/171] START max_depth=4, n_estimators=9..............................\n",
      "[CV 2/3; 66/171] END ............max_depth=4, n_estimators=9; total time=   0.7s\n",
      "[CV 3/3; 66/171] START max_depth=4, n_estimators=9..............................\n",
      "[CV 3/3; 66/171] END ............max_depth=4, n_estimators=9; total time=   0.8s\n",
      "[CV 1/3; 67/171] START max_depth=4, n_estimators=10.............................\n",
      "[CV 1/3; 67/171] END ...........max_depth=4, n_estimators=10; total time=   0.8s\n",
      "[CV 2/3; 67/171] START max_depth=4, n_estimators=10.............................\n",
      "[CV 2/3; 67/171] END ...........max_depth=4, n_estimators=10; total time=   0.8s\n",
      "[CV 3/3; 67/171] START max_depth=4, n_estimators=10.............................\n",
      "[CV 3/3; 67/171] END ...........max_depth=4, n_estimators=10; total time=   0.8s\n",
      "[CV 1/3; 68/171] START max_depth=4, n_estimators=11.............................\n",
      "[CV 1/3; 68/171] END ...........max_depth=4, n_estimators=11; total time=   0.9s\n",
      "[CV 2/3; 68/171] START max_depth=4, n_estimators=11.............................\n",
      "[CV 2/3; 68/171] END ...........max_depth=4, n_estimators=11; total time=   0.9s\n",
      "[CV 3/3; 68/171] START max_depth=4, n_estimators=11.............................\n",
      "[CV 3/3; 68/171] END ...........max_depth=4, n_estimators=11; total time=   0.9s\n",
      "[CV 1/3; 69/171] START max_depth=4, n_estimators=12.............................\n",
      "[CV 1/3; 69/171] END ...........max_depth=4, n_estimators=12; total time=   1.0s\n",
      "[CV 2/3; 69/171] START max_depth=4, n_estimators=12.............................\n",
      "[CV 2/3; 69/171] END ...........max_depth=4, n_estimators=12; total time=   1.0s\n",
      "[CV 3/3; 69/171] START max_depth=4, n_estimators=12.............................\n",
      "[CV 3/3; 69/171] END ...........max_depth=4, n_estimators=12; total time=   0.9s\n",
      "[CV 1/3; 70/171] START max_depth=4, n_estimators=13.............................\n",
      "[CV 1/3; 70/171] END ...........max_depth=4, n_estimators=13; total time=   1.0s\n",
      "[CV 2/3; 70/171] START max_depth=4, n_estimators=13.............................\n",
      "[CV 2/3; 70/171] END ...........max_depth=4, n_estimators=13; total time=   1.0s\n",
      "[CV 3/3; 70/171] START max_depth=4, n_estimators=13.............................\n",
      "[CV 3/3; 70/171] END ...........max_depth=4, n_estimators=13; total time=   1.0s\n",
      "[CV 1/3; 71/171] START max_depth=4, n_estimators=14.............................\n",
      "[CV 1/3; 71/171] END ...........max_depth=4, n_estimators=14; total time=   1.0s\n",
      "[CV 2/3; 71/171] START max_depth=4, n_estimators=14.............................\n",
      "[CV 2/3; 71/171] END ...........max_depth=4, n_estimators=14; total time=   1.1s\n",
      "[CV 3/3; 71/171] START max_depth=4, n_estimators=14.............................\n",
      "[CV 3/3; 71/171] END ...........max_depth=4, n_estimators=14; total time=   1.1s\n",
      "[CV 1/3; 72/171] START max_depth=4, n_estimators=15.............................\n",
      "[CV 1/3; 72/171] END ...........max_depth=4, n_estimators=15; total time=   1.1s\n",
      "[CV 2/3; 72/171] START max_depth=4, n_estimators=15.............................\n",
      "[CV 2/3; 72/171] END ...........max_depth=4, n_estimators=15; total time=   1.1s\n",
      "[CV 3/3; 72/171] START max_depth=4, n_estimators=15.............................\n",
      "[CV 3/3; 72/171] END ...........max_depth=4, n_estimators=15; total time=   1.1s\n",
      "[CV 1/3; 73/171] START max_depth=4, n_estimators=16.............................\n",
      "[CV 1/3; 73/171] END ...........max_depth=4, n_estimators=16; total time=   1.2s\n",
      "[CV 2/3; 73/171] START max_depth=4, n_estimators=16.............................\n",
      "[CV 2/3; 73/171] END ...........max_depth=4, n_estimators=16; total time=   1.3s\n",
      "[CV 3/3; 73/171] START max_depth=4, n_estimators=16.............................\n",
      "[CV 3/3; 73/171] END ...........max_depth=4, n_estimators=16; total time=   1.3s\n",
      "[CV 1/3; 74/171] START max_depth=4, n_estimators=17.............................\n",
      "[CV 1/3; 74/171] END ...........max_depth=4, n_estimators=17; total time=   1.3s\n",
      "[CV 2/3; 74/171] START max_depth=4, n_estimators=17.............................\n",
      "[CV 2/3; 74/171] END ...........max_depth=4, n_estimators=17; total time=   1.3s\n",
      "[CV 3/3; 74/171] START max_depth=4, n_estimators=17.............................\n",
      "[CV 3/3; 74/171] END ...........max_depth=4, n_estimators=17; total time=   1.4s\n",
      "[CV 1/3; 75/171] START max_depth=4, n_estimators=18.............................\n",
      "[CV 1/3; 75/171] END ...........max_depth=4, n_estimators=18; total time=   1.3s\n",
      "[CV 2/3; 75/171] START max_depth=4, n_estimators=18.............................\n",
      "[CV 2/3; 75/171] END ...........max_depth=4, n_estimators=18; total time=   1.3s\n",
      "[CV 3/3; 75/171] START max_depth=4, n_estimators=18.............................\n",
      "[CV 3/3; 75/171] END ...........max_depth=4, n_estimators=18; total time=   1.4s\n",
      "[CV 1/3; 76/171] START max_depth=4, n_estimators=19.............................\n",
      "[CV 1/3; 76/171] END ...........max_depth=4, n_estimators=19; total time=   1.5s\n",
      "[CV 2/3; 76/171] START max_depth=4, n_estimators=19.............................\n",
      "[CV 2/3; 76/171] END ...........max_depth=4, n_estimators=19; total time=   1.4s\n",
      "[CV 3/3; 76/171] START max_depth=4, n_estimators=19.............................\n",
      "[CV 3/3; 76/171] END ...........max_depth=4, n_estimators=19; total time=   1.4s\n",
      "[CV 1/3; 77/171] START max_depth=5, n_estimators=1..............................\n",
      "[CV 1/3; 77/171] END ............max_depth=5, n_estimators=1; total time=   0.2s\n",
      "[CV 2/3; 77/171] START max_depth=5, n_estimators=1..............................\n",
      "[CV 2/3; 77/171] END ............max_depth=5, n_estimators=1; total time=   0.2s\n",
      "[CV 3/3; 77/171] START max_depth=5, n_estimators=1..............................\n",
      "[CV 3/3; 77/171] END ............max_depth=5, n_estimators=1; total time=   0.2s\n",
      "[CV 1/3; 78/171] START max_depth=5, n_estimators=2..............................\n",
      "[CV 1/3; 78/171] END ............max_depth=5, n_estimators=2; total time=   0.3s\n",
      "[CV 2/3; 78/171] START max_depth=5, n_estimators=2..............................\n",
      "[CV 2/3; 78/171] END ............max_depth=5, n_estimators=2; total time=   0.3s\n",
      "[CV 3/3; 78/171] START max_depth=5, n_estimators=2..............................\n",
      "[CV 3/3; 78/171] END ............max_depth=5, n_estimators=2; total time=   0.3s\n",
      "[CV 1/3; 79/171] START max_depth=5, n_estimators=3..............................\n",
      "[CV 1/3; 79/171] END ............max_depth=5, n_estimators=3; total time=   0.4s\n",
      "[CV 2/3; 79/171] START max_depth=5, n_estimators=3..............................\n",
      "[CV 2/3; 79/171] END ............max_depth=5, n_estimators=3; total time=   0.4s\n",
      "[CV 3/3; 79/171] START max_depth=5, n_estimators=3..............................\n",
      "[CV 3/3; 79/171] END ............max_depth=5, n_estimators=3; total time=   0.4s\n",
      "[CV 1/3; 80/171] START max_depth=5, n_estimators=4..............................\n",
      "[CV 1/3; 80/171] END ............max_depth=5, n_estimators=4; total time=   0.5s\n",
      "[CV 2/3; 80/171] START max_depth=5, n_estimators=4..............................\n",
      "[CV 2/3; 80/171] END ............max_depth=5, n_estimators=4; total time=   0.4s\n",
      "[CV 3/3; 80/171] START max_depth=5, n_estimators=4..............................\n",
      "[CV 3/3; 80/171] END ............max_depth=5, n_estimators=4; total time=   0.4s\n",
      "[CV 1/3; 81/171] START max_depth=5, n_estimators=5..............................\n",
      "[CV 1/3; 81/171] END ............max_depth=5, n_estimators=5; total time=   0.5s\n",
      "[CV 2/3; 81/171] START max_depth=5, n_estimators=5..............................\n",
      "[CV 2/3; 81/171] END ............max_depth=5, n_estimators=5; total time=   0.5s\n",
      "[CV 3/3; 81/171] START max_depth=5, n_estimators=5..............................\n",
      "[CV 3/3; 81/171] END ............max_depth=5, n_estimators=5; total time=   0.5s\n",
      "[CV 1/3; 82/171] START max_depth=5, n_estimators=6..............................\n",
      "[CV 1/3; 82/171] END ............max_depth=5, n_estimators=6; total time=   0.6s\n",
      "[CV 2/3; 82/171] START max_depth=5, n_estimators=6..............................\n",
      "[CV 2/3; 82/171] END ............max_depth=5, n_estimators=6; total time=   0.6s\n",
      "[CV 3/3; 82/171] START max_depth=5, n_estimators=6..............................\n",
      "[CV 3/3; 82/171] END ............max_depth=5, n_estimators=6; total time=   0.6s\n",
      "[CV 1/3; 83/171] START max_depth=5, n_estimators=7..............................\n",
      "[CV 1/3; 83/171] END ............max_depth=5, n_estimators=7; total time=   0.7s\n",
      "[CV 2/3; 83/171] START max_depth=5, n_estimators=7..............................\n",
      "[CV 2/3; 83/171] END ............max_depth=5, n_estimators=7; total time=   0.7s\n",
      "[CV 3/3; 83/171] START max_depth=5, n_estimators=7..............................\n",
      "[CV 3/3; 83/171] END ............max_depth=5, n_estimators=7; total time=   0.7s\n",
      "[CV 1/3; 84/171] START max_depth=5, n_estimators=8..............................\n",
      "[CV 1/3; 84/171] END ............max_depth=5, n_estimators=8; total time=   0.8s\n",
      "[CV 2/3; 84/171] START max_depth=5, n_estimators=8..............................\n",
      "[CV 2/3; 84/171] END ............max_depth=5, n_estimators=8; total time=   0.8s\n",
      "[CV 3/3; 84/171] START max_depth=5, n_estimators=8..............................\n",
      "[CV 3/3; 84/171] END ............max_depth=5, n_estimators=8; total time=   0.8s\n",
      "[CV 1/3; 85/171] START max_depth=5, n_estimators=9..............................\n",
      "[CV 1/3; 85/171] END ............max_depth=5, n_estimators=9; total time=   0.9s\n",
      "[CV 2/3; 85/171] START max_depth=5, n_estimators=9..............................\n",
      "[CV 2/3; 85/171] END ............max_depth=5, n_estimators=9; total time=   0.9s\n",
      "[CV 3/3; 85/171] START max_depth=5, n_estimators=9..............................\n",
      "[CV 3/3; 85/171] END ............max_depth=5, n_estimators=9; total time=   0.8s\n",
      "[CV 1/3; 86/171] START max_depth=5, n_estimators=10.............................\n",
      "[CV 1/3; 86/171] END ...........max_depth=5, n_estimators=10; total time=   0.9s\n",
      "[CV 2/3; 86/171] START max_depth=5, n_estimators=10.............................\n",
      "[CV 2/3; 86/171] END ...........max_depth=5, n_estimators=10; total time=   0.9s\n",
      "[CV 3/3; 86/171] START max_depth=5, n_estimators=10.............................\n",
      "[CV 3/3; 86/171] END ...........max_depth=5, n_estimators=10; total time=   1.0s\n",
      "[CV 1/3; 87/171] START max_depth=5, n_estimators=11.............................\n",
      "[CV 1/3; 87/171] END ...........max_depth=5, n_estimators=11; total time=   1.0s\n",
      "[CV 2/3; 87/171] START max_depth=5, n_estimators=11.............................\n",
      "[CV 2/3; 87/171] END ...........max_depth=5, n_estimators=11; total time=   1.1s\n",
      "[CV 3/3; 87/171] START max_depth=5, n_estimators=11.............................\n",
      "[CV 3/3; 87/171] END ...........max_depth=5, n_estimators=11; total time=   1.4s\n",
      "[CV 1/3; 88/171] START max_depth=5, n_estimators=12.............................\n",
      "[CV 1/3; 88/171] END ...........max_depth=5, n_estimators=12; total time=   1.2s\n",
      "[CV 2/3; 88/171] START max_depth=5, n_estimators=12.............................\n",
      "[CV 2/3; 88/171] END ...........max_depth=5, n_estimators=12; total time=   1.3s\n",
      "[CV 3/3; 88/171] START max_depth=5, n_estimators=12.............................\n",
      "[CV 3/3; 88/171] END ...........max_depth=5, n_estimators=12; total time=   1.1s\n",
      "[CV 1/3; 89/171] START max_depth=5, n_estimators=13.............................\n",
      "[CV 1/3; 89/171] END ...........max_depth=5, n_estimators=13; total time=   1.2s\n",
      "[CV 2/3; 89/171] START max_depth=5, n_estimators=13.............................\n",
      "[CV 2/3; 89/171] END ...........max_depth=5, n_estimators=13; total time=   1.3s\n",
      "[CV 3/3; 89/171] START max_depth=5, n_estimators=13.............................\n",
      "[CV 3/3; 89/171] END ...........max_depth=5, n_estimators=13; total time=   1.2s\n",
      "[CV 1/3; 90/171] START max_depth=5, n_estimators=14.............................\n",
      "[CV 1/3; 90/171] END ...........max_depth=5, n_estimators=14; total time=   1.3s\n",
      "[CV 2/3; 90/171] START max_depth=5, n_estimators=14.............................\n",
      "[CV 2/3; 90/171] END ...........max_depth=5, n_estimators=14; total time=   1.3s\n",
      "[CV 3/3; 90/171] START max_depth=5, n_estimators=14.............................\n",
      "[CV 3/3; 90/171] END ...........max_depth=5, n_estimators=14; total time=   1.3s\n",
      "[CV 1/3; 91/171] START max_depth=5, n_estimators=15.............................\n",
      "[CV 1/3; 91/171] END ...........max_depth=5, n_estimators=15; total time=   1.5s\n",
      "[CV 2/3; 91/171] START max_depth=5, n_estimators=15.............................\n",
      "[CV 2/3; 91/171] END ...........max_depth=5, n_estimators=15; total time=   1.4s\n",
      "[CV 3/3; 91/171] START max_depth=5, n_estimators=15.............................\n",
      "[CV 3/3; 91/171] END ...........max_depth=5, n_estimators=15; total time=   1.4s\n",
      "[CV 1/3; 92/171] START max_depth=5, n_estimators=16.............................\n",
      "[CV 1/3; 92/171] END ...........max_depth=5, n_estimators=16; total time=   1.5s\n",
      "[CV 2/3; 92/171] START max_depth=5, n_estimators=16.............................\n",
      "[CV 2/3; 92/171] END ...........max_depth=5, n_estimators=16; total time=   1.4s\n",
      "[CV 3/3; 92/171] START max_depth=5, n_estimators=16.............................\n",
      "[CV 3/3; 92/171] END ...........max_depth=5, n_estimators=16; total time=   1.4s\n",
      "[CV 1/3; 93/171] START max_depth=5, n_estimators=17.............................\n",
      "[CV 1/3; 93/171] END ...........max_depth=5, n_estimators=17; total time=   1.5s\n",
      "[CV 2/3; 93/171] START max_depth=5, n_estimators=17.............................\n",
      "[CV 2/3; 93/171] END ...........max_depth=5, n_estimators=17; total time=   1.5s\n",
      "[CV 3/3; 93/171] START max_depth=5, n_estimators=17.............................\n",
      "[CV 3/3; 93/171] END ...........max_depth=5, n_estimators=17; total time=   1.5s\n",
      "[CV 1/3; 94/171] START max_depth=5, n_estimators=18.............................\n",
      "[CV 1/3; 94/171] END ...........max_depth=5, n_estimators=18; total time=   1.7s\n",
      "[CV 2/3; 94/171] START max_depth=5, n_estimators=18.............................\n",
      "[CV 2/3; 94/171] END ...........max_depth=5, n_estimators=18; total time=   1.6s\n",
      "[CV 3/3; 94/171] START max_depth=5, n_estimators=18.............................\n",
      "[CV 3/3; 94/171] END ...........max_depth=5, n_estimators=18; total time=   1.6s\n",
      "[CV 1/3; 95/171] START max_depth=5, n_estimators=19.............................\n",
      "[CV 1/3; 95/171] END ...........max_depth=5, n_estimators=19; total time=   1.7s\n",
      "[CV 2/3; 95/171] START max_depth=5, n_estimators=19.............................\n",
      "[CV 2/3; 95/171] END ...........max_depth=5, n_estimators=19; total time=   1.7s\n",
      "[CV 3/3; 95/171] START max_depth=5, n_estimators=19.............................\n",
      "[CV 3/3; 95/171] END ...........max_depth=5, n_estimators=19; total time=   1.7s\n",
      "[CV 1/3; 96/171] START max_depth=6, n_estimators=1..............................\n",
      "[CV 1/3; 96/171] END ............max_depth=6, n_estimators=1; total time=   0.2s\n",
      "[CV 2/3; 96/171] START max_depth=6, n_estimators=1..............................\n",
      "[CV 2/3; 96/171] END ............max_depth=6, n_estimators=1; total time=   0.2s\n",
      "[CV 3/3; 96/171] START max_depth=6, n_estimators=1..............................\n",
      "[CV 3/3; 96/171] END ............max_depth=6, n_estimators=1; total time=   0.2s\n",
      "[CV 1/3; 97/171] START max_depth=6, n_estimators=2..............................\n",
      "[CV 1/3; 97/171] END ............max_depth=6, n_estimators=2; total time=   0.3s\n",
      "[CV 2/3; 97/171] START max_depth=6, n_estimators=2..............................\n",
      "[CV 2/3; 97/171] END ............max_depth=6, n_estimators=2; total time=   0.3s\n",
      "[CV 3/3; 97/171] START max_depth=6, n_estimators=2..............................\n",
      "[CV 3/3; 97/171] END ............max_depth=6, n_estimators=2; total time=   0.3s\n",
      "[CV 1/3; 98/171] START max_depth=6, n_estimators=3..............................\n",
      "[CV 1/3; 98/171] END ............max_depth=6, n_estimators=3; total time=   0.4s\n",
      "[CV 2/3; 98/171] START max_depth=6, n_estimators=3..............................\n",
      "[CV 2/3; 98/171] END ............max_depth=6, n_estimators=3; total time=   0.4s\n",
      "[CV 3/3; 98/171] START max_depth=6, n_estimators=3..............................\n",
      "[CV 3/3; 98/171] END ............max_depth=6, n_estimators=3; total time=   0.4s\n",
      "[CV 1/3; 99/171] START max_depth=6, n_estimators=4..............................\n",
      "[CV 1/3; 99/171] END ............max_depth=6, n_estimators=4; total time=   0.5s\n",
      "[CV 2/3; 99/171] START max_depth=6, n_estimators=4..............................\n",
      "[CV 2/3; 99/171] END ............max_depth=6, n_estimators=4; total time=   0.5s\n",
      "[CV 3/3; 99/171] START max_depth=6, n_estimators=4..............................\n",
      "[CV 3/3; 99/171] END ............max_depth=6, n_estimators=4; total time=   0.5s\n",
      "[CV 1/3; 100/171] START max_depth=6, n_estimators=5.............................\n",
      "[CV 1/3; 100/171] END ...........max_depth=6, n_estimators=5; total time=   0.6s\n",
      "[CV 2/3; 100/171] START max_depth=6, n_estimators=5.............................\n",
      "[CV 2/3; 100/171] END ...........max_depth=6, n_estimators=5; total time=   0.6s\n",
      "[CV 3/3; 100/171] START max_depth=6, n_estimators=5.............................\n",
      "[CV 3/3; 100/171] END ...........max_depth=6, n_estimators=5; total time=   0.6s\n",
      "[CV 1/3; 101/171] START max_depth=6, n_estimators=6.............................\n",
      "[CV 1/3; 101/171] END ...........max_depth=6, n_estimators=6; total time=   0.7s\n",
      "[CV 2/3; 101/171] START max_depth=6, n_estimators=6.............................\n",
      "[CV 2/3; 101/171] END ...........max_depth=6, n_estimators=6; total time=   0.7s\n",
      "[CV 3/3; 101/171] START max_depth=6, n_estimators=6.............................\n",
      "[CV 3/3; 101/171] END ...........max_depth=6, n_estimators=6; total time=   0.7s\n",
      "[CV 1/3; 102/171] START max_depth=6, n_estimators=7.............................\n",
      "[CV 1/3; 102/171] END ...........max_depth=6, n_estimators=7; total time=   0.8s\n",
      "[CV 2/3; 102/171] START max_depth=6, n_estimators=7.............................\n",
      "[CV 2/3; 102/171] END ...........max_depth=6, n_estimators=7; total time=   0.8s\n",
      "[CV 3/3; 102/171] START max_depth=6, n_estimators=7.............................\n",
      "[CV 3/3; 102/171] END ...........max_depth=6, n_estimators=7; total time=   0.8s\n",
      "[CV 1/3; 103/171] START max_depth=6, n_estimators=8.............................\n",
      "[CV 1/3; 103/171] END ...........max_depth=6, n_estimators=8; total time=   1.0s\n",
      "[CV 2/3; 103/171] START max_depth=6, n_estimators=8.............................\n",
      "[CV 2/3; 103/171] END ...........max_depth=6, n_estimators=8; total time=   0.9s\n",
      "[CV 3/3; 103/171] START max_depth=6, n_estimators=8.............................\n",
      "[CV 3/3; 103/171] END ...........max_depth=6, n_estimators=8; total time=   0.9s\n",
      "[CV 1/3; 104/171] START max_depth=6, n_estimators=9.............................\n",
      "[CV 1/3; 104/171] END ...........max_depth=6, n_estimators=9; total time=   1.0s\n",
      "[CV 2/3; 104/171] START max_depth=6, n_estimators=9.............................\n",
      "[CV 2/3; 104/171] END ...........max_depth=6, n_estimators=9; total time=   1.0s\n",
      "[CV 3/3; 104/171] START max_depth=6, n_estimators=9.............................\n",
      "[CV 3/3; 104/171] END ...........max_depth=6, n_estimators=9; total time=   1.0s\n",
      "[CV 1/3; 105/171] START max_depth=6, n_estimators=10............................\n",
      "[CV 1/3; 105/171] END ..........max_depth=6, n_estimators=10; total time=   1.1s\n",
      "[CV 2/3; 105/171] START max_depth=6, n_estimators=10............................\n",
      "[CV 2/3; 105/171] END ..........max_depth=6, n_estimators=10; total time=   1.1s\n",
      "[CV 3/3; 105/171] START max_depth=6, n_estimators=10............................\n",
      "[CV 3/3; 105/171] END ..........max_depth=6, n_estimators=10; total time=   1.1s\n",
      "[CV 1/3; 106/171] START max_depth=6, n_estimators=11............................\n",
      "[CV 1/3; 106/171] END ..........max_depth=6, n_estimators=11; total time=   1.2s\n",
      "[CV 2/3; 106/171] START max_depth=6, n_estimators=11............................\n",
      "[CV 2/3; 106/171] END ..........max_depth=6, n_estimators=11; total time=   1.2s\n",
      "[CV 3/3; 106/171] START max_depth=6, n_estimators=11............................\n",
      "[CV 3/3; 106/171] END ..........max_depth=6, n_estimators=11; total time=   1.2s\n",
      "[CV 1/3; 107/171] START max_depth=6, n_estimators=12............................\n",
      "[CV 1/3; 107/171] END ..........max_depth=6, n_estimators=12; total time=   1.3s\n",
      "[CV 2/3; 107/171] START max_depth=6, n_estimators=12............................\n",
      "[CV 2/3; 107/171] END ..........max_depth=6, n_estimators=12; total time=   1.2s\n",
      "[CV 3/3; 107/171] START max_depth=6, n_estimators=12............................\n",
      "[CV 3/3; 107/171] END ..........max_depth=6, n_estimators=12; total time=   1.3s\n",
      "[CV 1/3; 108/171] START max_depth=6, n_estimators=13............................\n",
      "[CV 1/3; 108/171] END ..........max_depth=6, n_estimators=13; total time=   1.4s\n",
      "[CV 2/3; 108/171] START max_depth=6, n_estimators=13............................\n",
      "[CV 2/3; 108/171] END ..........max_depth=6, n_estimators=13; total time=   1.3s\n",
      "[CV 3/3; 108/171] START max_depth=6, n_estimators=13............................\n",
      "[CV 3/3; 108/171] END ..........max_depth=6, n_estimators=13; total time=   1.3s\n",
      "[CV 1/3; 109/171] START max_depth=6, n_estimators=14............................\n",
      "[CV 1/3; 109/171] END ..........max_depth=6, n_estimators=14; total time=   1.5s\n",
      "[CV 2/3; 109/171] START max_depth=6, n_estimators=14............................\n",
      "[CV 2/3; 109/171] END ..........max_depth=6, n_estimators=14; total time=   1.5s\n",
      "[CV 3/3; 109/171] START max_depth=6, n_estimators=14............................\n",
      "[CV 3/3; 109/171] END ..........max_depth=6, n_estimators=14; total time=   1.5s\n",
      "[CV 1/3; 110/171] START max_depth=6, n_estimators=15............................\n",
      "[CV 1/3; 110/171] END ..........max_depth=6, n_estimators=15; total time=   1.6s\n",
      "[CV 2/3; 110/171] START max_depth=6, n_estimators=15............................\n",
      "[CV 2/3; 110/171] END ..........max_depth=6, n_estimators=15; total time=   1.6s\n",
      "[CV 3/3; 110/171] START max_depth=6, n_estimators=15............................\n",
      "[CV 3/3; 110/171] END ..........max_depth=6, n_estimators=15; total time=   1.6s\n",
      "[CV 1/3; 111/171] START max_depth=6, n_estimators=16............................\n",
      "[CV 1/3; 111/171] END ..........max_depth=6, n_estimators=16; total time=   1.6s\n",
      "[CV 2/3; 111/171] START max_depth=6, n_estimators=16............................\n",
      "[CV 2/3; 111/171] END ..........max_depth=6, n_estimators=16; total time=   1.5s\n",
      "[CV 3/3; 111/171] START max_depth=6, n_estimators=16............................\n",
      "[CV 3/3; 111/171] END ..........max_depth=6, n_estimators=16; total time=   1.6s\n",
      "[CV 1/3; 112/171] START max_depth=6, n_estimators=17............................\n",
      "[CV 1/3; 112/171] END ..........max_depth=6, n_estimators=17; total time=   1.7s\n",
      "[CV 2/3; 112/171] START max_depth=6, n_estimators=17............................\n",
      "[CV 2/3; 112/171] END ..........max_depth=6, n_estimators=17; total time=   1.6s\n",
      "[CV 3/3; 112/171] START max_depth=6, n_estimators=17............................\n",
      "[CV 3/3; 112/171] END ..........max_depth=6, n_estimators=17; total time=   1.6s\n",
      "[CV 1/3; 113/171] START max_depth=6, n_estimators=18............................\n",
      "[CV 1/3; 113/171] END ..........max_depth=6, n_estimators=18; total time=   1.9s\n",
      "[CV 2/3; 113/171] START max_depth=6, n_estimators=18............................\n",
      "[CV 2/3; 113/171] END ..........max_depth=6, n_estimators=18; total time=   1.9s\n",
      "[CV 3/3; 113/171] START max_depth=6, n_estimators=18............................\n",
      "[CV 3/3; 113/171] END ..........max_depth=6, n_estimators=18; total time=   1.8s\n",
      "[CV 1/3; 114/171] START max_depth=6, n_estimators=19............................\n",
      "[CV 1/3; 114/171] END ..........max_depth=6, n_estimators=19; total time=   1.9s\n",
      "[CV 2/3; 114/171] START max_depth=6, n_estimators=19............................\n",
      "[CV 2/3; 114/171] END ..........max_depth=6, n_estimators=19; total time=   1.9s\n",
      "[CV 3/3; 114/171] START max_depth=6, n_estimators=19............................\n",
      "[CV 3/3; 114/171] END ..........max_depth=6, n_estimators=19; total time=   1.9s\n",
      "[CV 1/3; 115/171] START max_depth=7, n_estimators=1.............................\n",
      "[CV 1/3; 115/171] END ...........max_depth=7, n_estimators=1; total time=   0.2s\n",
      "[CV 2/3; 115/171] START max_depth=7, n_estimators=1.............................\n",
      "[CV 2/3; 115/171] END ...........max_depth=7, n_estimators=1; total time=   0.2s\n",
      "[CV 3/3; 115/171] START max_depth=7, n_estimators=1.............................\n",
      "[CV 3/3; 115/171] END ...........max_depth=7, n_estimators=1; total time=   0.2s\n",
      "[CV 1/3; 116/171] START max_depth=7, n_estimators=2.............................\n",
      "[CV 1/3; 116/171] END ...........max_depth=7, n_estimators=2; total time=   0.3s\n",
      "[CV 2/3; 116/171] START max_depth=7, n_estimators=2.............................\n",
      "[CV 2/3; 116/171] END ...........max_depth=7, n_estimators=2; total time=   0.3s\n",
      "[CV 3/3; 116/171] START max_depth=7, n_estimators=2.............................\n",
      "[CV 3/3; 116/171] END ...........max_depth=7, n_estimators=2; total time=   0.3s\n",
      "[CV 1/3; 117/171] START max_depth=7, n_estimators=3.............................\n",
      "[CV 1/3; 117/171] END ...........max_depth=7, n_estimators=3; total time=   0.4s\n",
      "[CV 2/3; 117/171] START max_depth=7, n_estimators=3.............................\n",
      "[CV 2/3; 117/171] END ...........max_depth=7, n_estimators=3; total time=   0.5s\n",
      "[CV 3/3; 117/171] START max_depth=7, n_estimators=3.............................\n",
      "[CV 3/3; 117/171] END ...........max_depth=7, n_estimators=3; total time=   0.5s\n",
      "[CV 1/3; 118/171] START max_depth=7, n_estimators=4.............................\n",
      "[CV 1/3; 118/171] END ...........max_depth=7, n_estimators=4; total time=   0.6s\n",
      "[CV 2/3; 118/171] START max_depth=7, n_estimators=4.............................\n",
      "[CV 2/3; 118/171] END ...........max_depth=7, n_estimators=4; total time=   0.6s\n",
      "[CV 3/3; 118/171] START max_depth=7, n_estimators=4.............................\n",
      "[CV 3/3; 118/171] END ...........max_depth=7, n_estimators=4; total time=   0.6s\n",
      "[CV 1/3; 119/171] START max_depth=7, n_estimators=5.............................\n",
      "[CV 1/3; 119/171] END ...........max_depth=7, n_estimators=5; total time=   0.7s\n",
      "[CV 2/3; 119/171] START max_depth=7, n_estimators=5.............................\n",
      "[CV 2/3; 119/171] END ...........max_depth=7, n_estimators=5; total time=   0.7s\n",
      "[CV 3/3; 119/171] START max_depth=7, n_estimators=5.............................\n",
      "[CV 3/3; 119/171] END ...........max_depth=7, n_estimators=5; total time=   0.7s\n",
      "[CV 1/3; 120/171] START max_depth=7, n_estimators=6.............................\n",
      "[CV 1/3; 120/171] END ...........max_depth=7, n_estimators=6; total time=   0.8s\n",
      "[CV 2/3; 120/171] START max_depth=7, n_estimators=6.............................\n",
      "[CV 2/3; 120/171] END ...........max_depth=7, n_estimators=6; total time=   0.8s\n",
      "[CV 3/3; 120/171] START max_depth=7, n_estimators=6.............................\n",
      "[CV 3/3; 120/171] END ...........max_depth=7, n_estimators=6; total time=   0.7s\n",
      "[CV 1/3; 121/171] START max_depth=7, n_estimators=7.............................\n",
      "[CV 1/3; 121/171] END ...........max_depth=7, n_estimators=7; total time=   0.9s\n",
      "[CV 2/3; 121/171] START max_depth=7, n_estimators=7.............................\n",
      "[CV 2/3; 121/171] END ...........max_depth=7, n_estimators=7; total time=   0.8s\n",
      "[CV 3/3; 121/171] START max_depth=7, n_estimators=7.............................\n",
      "[CV 3/3; 121/171] END ...........max_depth=7, n_estimators=7; total time=   0.8s\n",
      "[CV 1/3; 122/171] START max_depth=7, n_estimators=8.............................\n",
      "[CV 1/3; 122/171] END ...........max_depth=7, n_estimators=8; total time=   0.9s\n",
      "[CV 2/3; 122/171] START max_depth=7, n_estimators=8.............................\n",
      "[CV 2/3; 122/171] END ...........max_depth=7, n_estimators=8; total time=   0.9s\n",
      "[CV 3/3; 122/171] START max_depth=7, n_estimators=8.............................\n",
      "[CV 3/3; 122/171] END ...........max_depth=7, n_estimators=8; total time=   0.9s\n",
      "[CV 1/3; 123/171] START max_depth=7, n_estimators=9.............................\n",
      "[CV 1/3; 123/171] END ...........max_depth=7, n_estimators=9; total time=   1.1s\n",
      "[CV 2/3; 123/171] START max_depth=7, n_estimators=9.............................\n",
      "[CV 2/3; 123/171] END ...........max_depth=7, n_estimators=9; total time=   1.0s\n",
      "[CV 3/3; 123/171] START max_depth=7, n_estimators=9.............................\n",
      "[CV 3/3; 123/171] END ...........max_depth=7, n_estimators=9; total time=   1.0s\n",
      "[CV 1/3; 124/171] START max_depth=7, n_estimators=10............................\n",
      "[CV 1/3; 124/171] END ..........max_depth=7, n_estimators=10; total time=   1.2s\n",
      "[CV 2/3; 124/171] START max_depth=7, n_estimators=10............................\n",
      "[CV 2/3; 124/171] END ..........max_depth=7, n_estimators=10; total time=   1.3s\n",
      "[CV 3/3; 124/171] START max_depth=7, n_estimators=10............................\n",
      "[CV 3/3; 124/171] END ..........max_depth=7, n_estimators=10; total time=   1.1s\n",
      "[CV 1/3; 125/171] START max_depth=7, n_estimators=11............................\n",
      "[CV 1/3; 125/171] END ..........max_depth=7, n_estimators=11; total time=   1.3s\n",
      "[CV 2/3; 125/171] START max_depth=7, n_estimators=11............................\n",
      "[CV 2/3; 125/171] END ..........max_depth=7, n_estimators=11; total time=   1.3s\n",
      "[CV 3/3; 125/171] START max_depth=7, n_estimators=11............................\n",
      "[CV 3/3; 125/171] END ..........max_depth=7, n_estimators=11; total time=   1.3s\n",
      "[CV 1/3; 126/171] START max_depth=7, n_estimators=12............................\n",
      "[CV 1/3; 126/171] END ..........max_depth=7, n_estimators=12; total time=   1.4s\n",
      "[CV 2/3; 126/171] START max_depth=7, n_estimators=12............................\n",
      "[CV 2/3; 126/171] END ..........max_depth=7, n_estimators=12; total time=   1.3s\n",
      "[CV 3/3; 126/171] START max_depth=7, n_estimators=12............................\n",
      "[CV 3/3; 126/171] END ..........max_depth=7, n_estimators=12; total time=   1.4s\n",
      "[CV 1/3; 127/171] START max_depth=7, n_estimators=13............................\n",
      "[CV 1/3; 127/171] END ..........max_depth=7, n_estimators=13; total time=   1.6s\n",
      "[CV 2/3; 127/171] START max_depth=7, n_estimators=13............................\n",
      "[CV 2/3; 127/171] END ..........max_depth=7, n_estimators=13; total time=   1.6s\n",
      "[CV 3/3; 127/171] START max_depth=7, n_estimators=13............................\n",
      "[CV 3/3; 127/171] END ..........max_depth=7, n_estimators=13; total time=   1.6s\n",
      "[CV 1/3; 128/171] START max_depth=7, n_estimators=14............................\n",
      "[CV 1/3; 128/171] END ..........max_depth=7, n_estimators=14; total time=   1.8s\n",
      "[CV 2/3; 128/171] START max_depth=7, n_estimators=14............................\n",
      "[CV 2/3; 128/171] END ..........max_depth=7, n_estimators=14; total time=   1.8s\n",
      "[CV 3/3; 128/171] START max_depth=7, n_estimators=14............................\n",
      "[CV 3/3; 128/171] END ..........max_depth=7, n_estimators=14; total time=   1.7s\n",
      "[CV 1/3; 129/171] START max_depth=7, n_estimators=15............................\n",
      "[CV 1/3; 129/171] END ..........max_depth=7, n_estimators=15; total time=   1.8s\n",
      "[CV 2/3; 129/171] START max_depth=7, n_estimators=15............................\n",
      "[CV 2/3; 129/171] END ..........max_depth=7, n_estimators=15; total time=   1.7s\n",
      "[CV 3/3; 129/171] START max_depth=7, n_estimators=15............................\n",
      "[CV 3/3; 129/171] END ..........max_depth=7, n_estimators=15; total time=   1.7s\n",
      "[CV 1/3; 130/171] START max_depth=7, n_estimators=16............................\n",
      "[CV 1/3; 130/171] END ..........max_depth=7, n_estimators=16; total time=   1.8s\n",
      "[CV 2/3; 130/171] START max_depth=7, n_estimators=16............................\n",
      "[CV 2/3; 130/171] END ..........max_depth=7, n_estimators=16; total time=   1.9s\n",
      "[CV 3/3; 130/171] START max_depth=7, n_estimators=16............................\n",
      "[CV 3/3; 130/171] END ..........max_depth=7, n_estimators=16; total time=   1.9s\n",
      "[CV 1/3; 131/171] START max_depth=7, n_estimators=17............................\n",
      "[CV 1/3; 131/171] END ..........max_depth=7, n_estimators=17; total time=   1.9s\n",
      "[CV 2/3; 131/171] START max_depth=7, n_estimators=17............................\n",
      "[CV 2/3; 131/171] END ..........max_depth=7, n_estimators=17; total time=   1.9s\n",
      "[CV 3/3; 131/171] START max_depth=7, n_estimators=17............................\n",
      "[CV 3/3; 131/171] END ..........max_depth=7, n_estimators=17; total time=   2.0s\n",
      "[CV 1/3; 132/171] START max_depth=7, n_estimators=18............................\n",
      "[CV 1/3; 132/171] END ..........max_depth=7, n_estimators=18; total time=   2.1s\n",
      "[CV 2/3; 132/171] START max_depth=7, n_estimators=18............................\n",
      "[CV 2/3; 132/171] END ..........max_depth=7, n_estimators=18; total time=   2.1s\n",
      "[CV 3/3; 132/171] START max_depth=7, n_estimators=18............................\n",
      "[CV 3/3; 132/171] END ..........max_depth=7, n_estimators=18; total time=   2.0s\n",
      "[CV 1/3; 133/171] START max_depth=7, n_estimators=19............................\n",
      "[CV 1/3; 133/171] END ..........max_depth=7, n_estimators=19; total time=   2.4s\n",
      "[CV 2/3; 133/171] START max_depth=7, n_estimators=19............................\n",
      "[CV 2/3; 133/171] END ..........max_depth=7, n_estimators=19; total time=   2.4s\n",
      "[CV 3/3; 133/171] START max_depth=7, n_estimators=19............................\n",
      "[CV 3/3; 133/171] END ..........max_depth=7, n_estimators=19; total time=   2.2s\n",
      "[CV 1/3; 134/171] START max_depth=8, n_estimators=1.............................\n",
      "[CV 1/3; 134/171] END ...........max_depth=8, n_estimators=1; total time=   0.2s\n",
      "[CV 2/3; 134/171] START max_depth=8, n_estimators=1.............................\n",
      "[CV 2/3; 134/171] END ...........max_depth=8, n_estimators=1; total time=   0.3s\n",
      "[CV 3/3; 134/171] START max_depth=8, n_estimators=1.............................\n",
      "[CV 3/3; 134/171] END ...........max_depth=8, n_estimators=1; total time=   0.2s\n",
      "[CV 1/3; 135/171] START max_depth=8, n_estimators=2.............................\n",
      "[CV 1/3; 135/171] END ...........max_depth=8, n_estimators=2; total time=   0.4s\n",
      "[CV 2/3; 135/171] START max_depth=8, n_estimators=2.............................\n",
      "[CV 2/3; 135/171] END ...........max_depth=8, n_estimators=2; total time=   0.4s\n",
      "[CV 3/3; 135/171] START max_depth=8, n_estimators=2.............................\n",
      "[CV 3/3; 135/171] END ...........max_depth=8, n_estimators=2; total time=   0.4s\n",
      "[CV 1/3; 136/171] START max_depth=8, n_estimators=3.............................\n",
      "[CV 1/3; 136/171] END ...........max_depth=8, n_estimators=3; total time=   0.5s\n",
      "[CV 2/3; 136/171] START max_depth=8, n_estimators=3.............................\n",
      "[CV 2/3; 136/171] END ...........max_depth=8, n_estimators=3; total time=   0.5s\n",
      "[CV 3/3; 136/171] START max_depth=8, n_estimators=3.............................\n",
      "[CV 3/3; 136/171] END ...........max_depth=8, n_estimators=3; total time=   0.5s\n",
      "[CV 1/3; 137/171] START max_depth=8, n_estimators=4.............................\n",
      "[CV 1/3; 137/171] END ...........max_depth=8, n_estimators=4; total time=   0.6s\n",
      "[CV 2/3; 137/171] START max_depth=8, n_estimators=4.............................\n",
      "[CV 2/3; 137/171] END ...........max_depth=8, n_estimators=4; total time=   0.6s\n",
      "[CV 3/3; 137/171] START max_depth=8, n_estimators=4.............................\n",
      "[CV 3/3; 137/171] END ...........max_depth=8, n_estimators=4; total time=   0.6s\n",
      "[CV 1/3; 138/171] START max_depth=8, n_estimators=5.............................\n",
      "[CV 1/3; 138/171] END ...........max_depth=8, n_estimators=5; total time=   0.8s\n",
      "[CV 2/3; 138/171] START max_depth=8, n_estimators=5.............................\n",
      "[CV 2/3; 138/171] END ...........max_depth=8, n_estimators=5; total time=   0.7s\n",
      "[CV 3/3; 138/171] START max_depth=8, n_estimators=5.............................\n",
      "[CV 3/3; 138/171] END ...........max_depth=8, n_estimators=5; total time=   0.7s\n",
      "[CV 1/3; 139/171] START max_depth=8, n_estimators=6.............................\n",
      "[CV 1/3; 139/171] END ...........max_depth=8, n_estimators=6; total time=   0.9s\n",
      "[CV 2/3; 139/171] START max_depth=8, n_estimators=6.............................\n",
      "[CV 2/3; 139/171] END ...........max_depth=8, n_estimators=6; total time=   0.9s\n",
      "[CV 3/3; 139/171] START max_depth=8, n_estimators=6.............................\n",
      "[CV 3/3; 139/171] END ...........max_depth=8, n_estimators=6; total time=   0.9s\n",
      "[CV 1/3; 140/171] START max_depth=8, n_estimators=7.............................\n",
      "[CV 1/3; 140/171] END ...........max_depth=8, n_estimators=7; total time=   1.1s\n",
      "[CV 2/3; 140/171] START max_depth=8, n_estimators=7.............................\n",
      "[CV 2/3; 140/171] END ...........max_depth=8, n_estimators=7; total time=   1.0s\n",
      "[CV 3/3; 140/171] START max_depth=8, n_estimators=7.............................\n",
      "[CV 3/3; 140/171] END ...........max_depth=8, n_estimators=7; total time=   1.0s\n",
      "[CV 1/3; 141/171] START max_depth=8, n_estimators=8.............................\n",
      "[CV 1/3; 141/171] END ...........max_depth=8, n_estimators=8; total time=   1.3s\n",
      "[CV 2/3; 141/171] START max_depth=8, n_estimators=8.............................\n",
      "[CV 2/3; 141/171] END ...........max_depth=8, n_estimators=8; total time=   1.6s\n",
      "[CV 3/3; 141/171] START max_depth=8, n_estimators=8.............................\n",
      "[CV 3/3; 141/171] END ...........max_depth=8, n_estimators=8; total time=   1.3s\n",
      "[CV 1/3; 142/171] START max_depth=8, n_estimators=9.............................\n",
      "[CV 1/3; 142/171] END ...........max_depth=8, n_estimators=9; total time=   1.3s\n",
      "[CV 2/3; 142/171] START max_depth=8, n_estimators=9.............................\n",
      "[CV 2/3; 142/171] END ...........max_depth=8, n_estimators=9; total time=   1.2s\n",
      "[CV 3/3; 142/171] START max_depth=8, n_estimators=9.............................\n",
      "[CV 3/3; 142/171] END ...........max_depth=8, n_estimators=9; total time=   1.2s\n",
      "[CV 1/3; 143/171] START max_depth=8, n_estimators=10............................\n",
      "[CV 1/3; 143/171] END ..........max_depth=8, n_estimators=10; total time=   1.4s\n",
      "[CV 2/3; 143/171] START max_depth=8, n_estimators=10............................\n",
      "[CV 2/3; 143/171] END ..........max_depth=8, n_estimators=10; total time=   1.4s\n",
      "[CV 3/3; 143/171] START max_depth=8, n_estimators=10............................\n",
      "[CV 3/3; 143/171] END ..........max_depth=8, n_estimators=10; total time=   1.4s\n",
      "[CV 1/3; 144/171] START max_depth=8, n_estimators=11............................\n",
      "[CV 1/3; 144/171] END ..........max_depth=8, n_estimators=11; total time=   1.5s\n",
      "[CV 2/3; 144/171] START max_depth=8, n_estimators=11............................\n",
      "[CV 2/3; 144/171] END ..........max_depth=8, n_estimators=11; total time=   1.5s\n",
      "[CV 3/3; 144/171] START max_depth=8, n_estimators=11............................\n",
      "[CV 3/3; 144/171] END ..........max_depth=8, n_estimators=11; total time=   1.5s\n",
      "[CV 1/3; 145/171] START max_depth=8, n_estimators=12............................\n",
      "[CV 1/3; 145/171] END ..........max_depth=8, n_estimators=12; total time=   1.9s\n",
      "[CV 2/3; 145/171] START max_depth=8, n_estimators=12............................\n",
      "[CV 2/3; 145/171] END ..........max_depth=8, n_estimators=12; total time=   1.6s\n",
      "[CV 3/3; 145/171] START max_depth=8, n_estimators=12............................\n",
      "[CV 3/3; 145/171] END ..........max_depth=8, n_estimators=12; total time=   1.6s\n",
      "[CV 1/3; 146/171] START max_depth=8, n_estimators=13............................\n",
      "[CV 1/3; 146/171] END ..........max_depth=8, n_estimators=13; total time=   1.7s\n",
      "[CV 2/3; 146/171] START max_depth=8, n_estimators=13............................\n",
      "[CV 2/3; 146/171] END ..........max_depth=8, n_estimators=13; total time=   1.7s\n",
      "[CV 3/3; 146/171] START max_depth=8, n_estimators=13............................\n",
      "[CV 3/3; 146/171] END ..........max_depth=8, n_estimators=13; total time=   1.7s\n",
      "[CV 1/3; 147/171] START max_depth=8, n_estimators=14............................\n",
      "[CV 1/3; 147/171] END ..........max_depth=8, n_estimators=14; total time=   2.0s\n",
      "[CV 2/3; 147/171] START max_depth=8, n_estimators=14............................\n",
      "[CV 2/3; 147/171] END ..........max_depth=8, n_estimators=14; total time=   2.1s\n",
      "[CV 3/3; 147/171] START max_depth=8, n_estimators=14............................\n",
      "[CV 3/3; 147/171] END ..........max_depth=8, n_estimators=14; total time=   2.0s\n",
      "[CV 1/3; 148/171] START max_depth=8, n_estimators=15............................\n",
      "[CV 1/3; 148/171] END ..........max_depth=8, n_estimators=15; total time=   2.0s\n",
      "[CV 2/3; 148/171] START max_depth=8, n_estimators=15............................\n",
      "[CV 2/3; 148/171] END ..........max_depth=8, n_estimators=15; total time=   2.0s\n",
      "[CV 3/3; 148/171] START max_depth=8, n_estimators=15............................\n",
      "[CV 3/3; 148/171] END ..........max_depth=8, n_estimators=15; total time=   2.1s\n",
      "[CV 1/3; 149/171] START max_depth=8, n_estimators=16............................\n",
      "[CV 1/3; 149/171] END ..........max_depth=8, n_estimators=16; total time=   2.1s\n",
      "[CV 2/3; 149/171] START max_depth=8, n_estimators=16............................\n",
      "[CV 2/3; 149/171] END ..........max_depth=8, n_estimators=16; total time=   2.1s\n",
      "[CV 3/3; 149/171] START max_depth=8, n_estimators=16............................\n",
      "[CV 3/3; 149/171] END ..........max_depth=8, n_estimators=16; total time=   2.1s\n",
      "[CV 1/3; 150/171] START max_depth=8, n_estimators=17............................\n",
      "[CV 1/3; 150/171] END ..........max_depth=8, n_estimators=17; total time=   2.2s\n",
      "[CV 2/3; 150/171] START max_depth=8, n_estimators=17............................\n",
      "[CV 2/3; 150/171] END ..........max_depth=8, n_estimators=17; total time=   2.3s\n",
      "[CV 3/3; 150/171] START max_depth=8, n_estimators=17............................\n",
      "[CV 3/3; 150/171] END ..........max_depth=8, n_estimators=17; total time=   2.2s\n",
      "[CV 1/3; 151/171] START max_depth=8, n_estimators=18............................\n",
      "[CV 1/3; 151/171] END ..........max_depth=8, n_estimators=18; total time=   2.3s\n",
      "[CV 2/3; 151/171] START max_depth=8, n_estimators=18............................\n",
      "[CV 2/3; 151/171] END ..........max_depth=8, n_estimators=18; total time=   2.3s\n",
      "[CV 3/3; 151/171] START max_depth=8, n_estimators=18............................\n",
      "[CV 3/3; 151/171] END ..........max_depth=8, n_estimators=18; total time=   2.3s\n",
      "[CV 1/3; 152/171] START max_depth=8, n_estimators=19............................\n",
      "[CV 1/3; 152/171] END ..........max_depth=8, n_estimators=19; total time=   2.4s\n",
      "[CV 2/3; 152/171] START max_depth=8, n_estimators=19............................\n",
      "[CV 2/3; 152/171] END ..........max_depth=8, n_estimators=19; total time=   2.6s\n",
      "[CV 3/3; 152/171] START max_depth=8, n_estimators=19............................\n",
      "[CV 3/3; 152/171] END ..........max_depth=8, n_estimators=19; total time=   2.5s\n",
      "[CV 1/3; 153/171] START max_depth=9, n_estimators=1.............................\n",
      "[CV 1/3; 153/171] END ...........max_depth=9, n_estimators=1; total time=   0.2s\n",
      "[CV 2/3; 153/171] START max_depth=9, n_estimators=1.............................\n",
      "[CV 2/3; 153/171] END ...........max_depth=9, n_estimators=1; total time=   0.2s\n",
      "[CV 3/3; 153/171] START max_depth=9, n_estimators=1.............................\n",
      "[CV 3/3; 153/171] END ...........max_depth=9, n_estimators=1; total time=   0.3s\n",
      "[CV 1/3; 154/171] START max_depth=9, n_estimators=2.............................\n",
      "[CV 1/3; 154/171] END ...........max_depth=9, n_estimators=2; total time=   0.4s\n",
      "[CV 2/3; 154/171] START max_depth=9, n_estimators=2.............................\n",
      "[CV 2/3; 154/171] END ...........max_depth=9, n_estimators=2; total time=   0.5s\n",
      "[CV 3/3; 154/171] START max_depth=9, n_estimators=2.............................\n",
      "[CV 3/3; 154/171] END ...........max_depth=9, n_estimators=2; total time=   0.4s\n",
      "[CV 1/3; 155/171] START max_depth=9, n_estimators=3.............................\n",
      "[CV 1/3; 155/171] END ...........max_depth=9, n_estimators=3; total time=   0.6s\n",
      "[CV 2/3; 155/171] START max_depth=9, n_estimators=3.............................\n",
      "[CV 2/3; 155/171] END ...........max_depth=9, n_estimators=3; total time=   0.6s\n",
      "[CV 3/3; 155/171] START max_depth=9, n_estimators=3.............................\n",
      "[CV 3/3; 155/171] END ...........max_depth=9, n_estimators=3; total time=   0.6s\n",
      "[CV 1/3; 156/171] START max_depth=9, n_estimators=4.............................\n",
      "[CV 1/3; 156/171] END ...........max_depth=9, n_estimators=4; total time=   0.7s\n",
      "[CV 2/3; 156/171] START max_depth=9, n_estimators=4.............................\n",
      "[CV 2/3; 156/171] END ...........max_depth=9, n_estimators=4; total time=   0.7s\n",
      "[CV 3/3; 156/171] START max_depth=9, n_estimators=4.............................\n",
      "[CV 3/3; 156/171] END ...........max_depth=9, n_estimators=4; total time=   0.7s\n",
      "[CV 1/3; 157/171] START max_depth=9, n_estimators=5.............................\n",
      "[CV 1/3; 157/171] END ...........max_depth=9, n_estimators=5; total time=   0.9s\n",
      "[CV 2/3; 157/171] START max_depth=9, n_estimators=5.............................\n",
      "[CV 2/3; 157/171] END ...........max_depth=9, n_estimators=5; total time=   0.9s\n",
      "[CV 3/3; 157/171] START max_depth=9, n_estimators=5.............................\n",
      "[CV 3/3; 157/171] END ...........max_depth=9, n_estimators=5; total time=   0.9s\n",
      "[CV 1/3; 158/171] START max_depth=9, n_estimators=6.............................\n",
      "[CV 1/3; 158/171] END ...........max_depth=9, n_estimators=6; total time=   1.0s\n",
      "[CV 2/3; 158/171] START max_depth=9, n_estimators=6.............................\n",
      "[CV 2/3; 158/171] END ...........max_depth=9, n_estimators=6; total time=   1.0s\n",
      "[CV 3/3; 158/171] START max_depth=9, n_estimators=6.............................\n",
      "[CV 3/3; 158/171] END ...........max_depth=9, n_estimators=6; total time=   1.0s\n",
      "[CV 1/3; 159/171] START max_depth=9, n_estimators=7.............................\n",
      "[CV 1/3; 159/171] END ...........max_depth=9, n_estimators=7; total time=   1.1s\n",
      "[CV 2/3; 159/171] START max_depth=9, n_estimators=7.............................\n",
      "[CV 2/3; 159/171] END ...........max_depth=9, n_estimators=7; total time=   1.1s\n",
      "[CV 3/3; 159/171] START max_depth=9, n_estimators=7.............................\n",
      "[CV 3/3; 159/171] END ...........max_depth=9, n_estimators=7; total time=   1.0s\n",
      "[CV 1/3; 160/171] START max_depth=9, n_estimators=8.............................\n",
      "[CV 1/3; 160/171] END ...........max_depth=9, n_estimators=8; total time=   1.2s\n",
      "[CV 2/3; 160/171] START max_depth=9, n_estimators=8.............................\n",
      "[CV 2/3; 160/171] END ...........max_depth=9, n_estimators=8; total time=   1.2s\n",
      "[CV 3/3; 160/171] START max_depth=9, n_estimators=8.............................\n",
      "[CV 3/3; 160/171] END ...........max_depth=9, n_estimators=8; total time=   1.2s\n",
      "[CV 1/3; 161/171] START max_depth=9, n_estimators=9.............................\n",
      "[CV 1/3; 161/171] END ...........max_depth=9, n_estimators=9; total time=   1.3s\n",
      "[CV 2/3; 161/171] START max_depth=9, n_estimators=9.............................\n",
      "[CV 2/3; 161/171] END ...........max_depth=9, n_estimators=9; total time=   1.4s\n",
      "[CV 3/3; 161/171] START max_depth=9, n_estimators=9.............................\n",
      "[CV 3/3; 161/171] END ...........max_depth=9, n_estimators=9; total time=   1.6s\n",
      "[CV 1/3; 162/171] START max_depth=9, n_estimators=10............................\n",
      "[CV 1/3; 162/171] END ..........max_depth=9, n_estimators=10; total time=   1.5s\n",
      "[CV 2/3; 162/171] START max_depth=9, n_estimators=10............................\n",
      "[CV 2/3; 162/171] END ..........max_depth=9, n_estimators=10; total time=   1.5s\n",
      "[CV 3/3; 162/171] START max_depth=9, n_estimators=10............................\n",
      "[CV 3/3; 162/171] END ..........max_depth=9, n_estimators=10; total time=   1.5s\n",
      "[CV 1/3; 163/171] START max_depth=9, n_estimators=11............................\n",
      "[CV 1/3; 163/171] END ..........max_depth=9, n_estimators=11; total time=   1.6s\n",
      "[CV 2/3; 163/171] START max_depth=9, n_estimators=11............................\n",
      "[CV 2/3; 163/171] END ..........max_depth=9, n_estimators=11; total time=   1.6s\n",
      "[CV 3/3; 163/171] START max_depth=9, n_estimators=11............................\n",
      "[CV 3/3; 163/171] END ..........max_depth=9, n_estimators=11; total time=   1.7s\n",
      "[CV 1/3; 164/171] START max_depth=9, n_estimators=12............................\n",
      "[CV 1/3; 164/171] END ..........max_depth=9, n_estimators=12; total time=   1.8s\n",
      "[CV 2/3; 164/171] START max_depth=9, n_estimators=12............................\n",
      "[CV 2/3; 164/171] END ..........max_depth=9, n_estimators=12; total time=   1.8s\n",
      "[CV 3/3; 164/171] START max_depth=9, n_estimators=12............................\n",
      "[CV 3/3; 164/171] END ..........max_depth=9, n_estimators=12; total time=   1.7s\n",
      "[CV 1/3; 165/171] START max_depth=9, n_estimators=13............................\n",
      "[CV 1/3; 165/171] END ..........max_depth=9, n_estimators=13; total time=   1.9s\n",
      "[CV 2/3; 165/171] START max_depth=9, n_estimators=13............................\n",
      "[CV 2/3; 165/171] END ..........max_depth=9, n_estimators=13; total time=   1.9s\n",
      "[CV 3/3; 165/171] START max_depth=9, n_estimators=13............................\n",
      "[CV 3/3; 165/171] END ..........max_depth=9, n_estimators=13; total time=   1.9s\n",
      "[CV 1/3; 166/171] START max_depth=9, n_estimators=14............................\n",
      "[CV 1/3; 166/171] END ..........max_depth=9, n_estimators=14; total time=   2.0s\n",
      "[CV 2/3; 166/171] START max_depth=9, n_estimators=14............................\n",
      "[CV 2/3; 166/171] END ..........max_depth=9, n_estimators=14; total time=   2.0s\n",
      "[CV 3/3; 166/171] START max_depth=9, n_estimators=14............................\n",
      "[CV 3/3; 166/171] END ..........max_depth=9, n_estimators=14; total time=   2.0s\n",
      "[CV 1/3; 167/171] START max_depth=9, n_estimators=15............................\n",
      "[CV 1/3; 167/171] END ..........max_depth=9, n_estimators=15; total time=   2.2s\n",
      "[CV 2/3; 167/171] START max_depth=9, n_estimators=15............................\n",
      "[CV 2/3; 167/171] END ..........max_depth=9, n_estimators=15; total time=   2.2s\n",
      "[CV 3/3; 167/171] START max_depth=9, n_estimators=15............................\n",
      "[CV 3/3; 167/171] END ..........max_depth=9, n_estimators=15; total time=   2.2s\n",
      "[CV 1/3; 168/171] START max_depth=9, n_estimators=16............................\n",
      "[CV 1/3; 168/171] END ..........max_depth=9, n_estimators=16; total time=   2.4s\n",
      "[CV 2/3; 168/171] START max_depth=9, n_estimators=16............................\n",
      "[CV 2/3; 168/171] END ..........max_depth=9, n_estimators=16; total time=   2.3s\n",
      "[CV 3/3; 168/171] START max_depth=9, n_estimators=16............................\n",
      "[CV 3/3; 168/171] END ..........max_depth=9, n_estimators=16; total time=   2.5s\n",
      "[CV 1/3; 169/171] START max_depth=9, n_estimators=17............................\n",
      "[CV 1/3; 169/171] END ..........max_depth=9, n_estimators=17; total time=   2.7s\n",
      "[CV 2/3; 169/171] START max_depth=9, n_estimators=17............................\n",
      "[CV 2/3; 169/171] END ..........max_depth=9, n_estimators=17; total time=   2.7s\n",
      "[CV 3/3; 169/171] START max_depth=9, n_estimators=17............................\n",
      "[CV 3/3; 169/171] END ..........max_depth=9, n_estimators=17; total time=   2.7s\n",
      "[CV 1/3; 170/171] START max_depth=9, n_estimators=18............................\n",
      "[CV 1/3; 170/171] END ..........max_depth=9, n_estimators=18; total time=   2.6s\n",
      "[CV 2/3; 170/171] START max_depth=9, n_estimators=18............................\n",
      "[CV 2/3; 170/171] END ..........max_depth=9, n_estimators=18; total time=   2.6s\n",
      "[CV 3/3; 170/171] START max_depth=9, n_estimators=18............................\n",
      "[CV 3/3; 170/171] END ..........max_depth=9, n_estimators=18; total time=   2.7s\n",
      "[CV 1/3; 171/171] START max_depth=9, n_estimators=19............................\n",
      "[CV 1/3; 171/171] END ..........max_depth=9, n_estimators=19; total time=   3.2s\n",
      "[CV 2/3; 171/171] START max_depth=9, n_estimators=19............................\n",
      "[CV 2/3; 171/171] END ..........max_depth=9, n_estimators=19; total time=   2.9s\n",
      "[CV 3/3; 171/171] START max_depth=9, n_estimators=19............................\n",
      "[CV 3/3; 171/171] END ..........max_depth=9, n_estimators=19; total time=   2.9s\n",
      "{'max_depth': 9, 'n_estimators': 1}\n",
      "CPU times: user 8min 3s, sys: 0 ns, total: 8min 3s\n",
      "Wall time: 8min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': range(1, 10, 1),\n",
    "    'n_estimators': range(1, 20, 1)}\n",
    "\n",
    "model_RTC = RandomForestClassifier(random_state=12345) \n",
    "\n",
    "model_RTC_gs = GridSearchCV(estimator=model_RTC, \n",
    "                     param_grid=param_grid,\n",
    "                     cv=3, \n",
    "                     n_jobs=-1, \n",
    "                     verbose=10,\n",
    "                     scoring='f1')\n",
    "model_RTC_gs.fit(features_train, target_train)\n",
    "print(model_RTC_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 для RandomForestClassifier: 0.07508427254846584\n"
     ]
    }
   ],
   "source": [
    "model_RTC = RandomForestClassifier(max_depth=9, n_estimators=1, random_state=12345)\n",
    "model_RTC.fit(features_train, target_train)\n",
    "scores_RTC = cross_val_score(model_RTC,\n",
    "                            features_train,\n",
    "                            target_train,\n",
    "                            cv=3,\n",
    "                            n_jobs=-1,\n",
    "                            scoring='f1') \n",
    "\n",
    "f1_RTC = scores_RTC.mean()\n",
    "print('f1 для RandomForestClassifier:', f1_RTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 49 candidates, totalling 147 fits\n",
      "[CV 1/3; 1/49] START max_depth=1................................................\n",
      "[CV 1/3; 1/49] END ..............................max_depth=1; total time=   6.5s\n",
      "[CV 2/3; 1/49] START max_depth=1................................................\n",
      "[CV 2/3; 1/49] END ..............................max_depth=1; total time=   7.0s\n",
      "[CV 3/3; 1/49] START max_depth=1................................................\n",
      "[CV 3/3; 1/49] END ..............................max_depth=1; total time=   6.7s\n",
      "[CV 1/3; 2/49] START max_depth=2................................................\n",
      "[CV 1/3; 2/49] END ..............................max_depth=2; total time=   6.7s\n",
      "[CV 2/3; 2/49] START max_depth=2................................................\n",
      "[CV 2/3; 2/49] END ..............................max_depth=2; total time=   6.6s\n",
      "[CV 3/3; 2/49] START max_depth=2................................................\n",
      "[CV 3/3; 2/49] END ..............................max_depth=2; total time=   6.9s\n",
      "[CV 1/3; 3/49] START max_depth=3................................................\n",
      "[CV 1/3; 3/49] END ..............................max_depth=3; total time=   7.2s\n",
      "[CV 2/3; 3/49] START max_depth=3................................................\n",
      "[CV 2/3; 3/49] END ..............................max_depth=3; total time=   7.2s\n",
      "[CV 3/3; 3/49] START max_depth=3................................................\n",
      "[CV 3/3; 3/49] END ..............................max_depth=3; total time=   6.9s\n",
      "[CV 1/3; 4/49] START max_depth=4................................................\n",
      "[CV 1/3; 4/49] END ..............................max_depth=4; total time=   7.4s\n",
      "[CV 2/3; 4/49] START max_depth=4................................................\n",
      "[CV 2/3; 4/49] END ..............................max_depth=4; total time=   7.6s\n",
      "[CV 3/3; 4/49] START max_depth=4................................................\n",
      "[CV 3/3; 4/49] END ..............................max_depth=4; total time=   7.9s\n",
      "[CV 1/3; 5/49] START max_depth=5................................................\n",
      "[CV 1/3; 5/49] END ..............................max_depth=5; total time=   7.8s\n",
      "[CV 2/3; 5/49] START max_depth=5................................................\n",
      "[CV 2/3; 5/49] END ..............................max_depth=5; total time=   7.9s\n",
      "[CV 3/3; 5/49] START max_depth=5................................................\n",
      "[CV 3/3; 5/49] END ..............................max_depth=5; total time=   7.8s\n",
      "[CV 1/3; 6/49] START max_depth=6................................................\n",
      "[CV 1/3; 6/49] END ..............................max_depth=6; total time=   8.1s\n",
      "[CV 2/3; 6/49] START max_depth=6................................................\n",
      "[CV 2/3; 6/49] END ..............................max_depth=6; total time=   8.3s\n",
      "[CV 3/3; 6/49] START max_depth=6................................................\n",
      "[CV 3/3; 6/49] END ..............................max_depth=6; total time=   8.3s\n",
      "[CV 1/3; 7/49] START max_depth=7................................................\n",
      "[CV 1/3; 7/49] END ..............................max_depth=7; total time=   9.0s\n",
      "[CV 2/3; 7/49] START max_depth=7................................................\n",
      "[CV 2/3; 7/49] END ..............................max_depth=7; total time=   8.4s\n",
      "[CV 3/3; 7/49] START max_depth=7................................................\n",
      "[CV 3/3; 7/49] END ..............................max_depth=7; total time=   8.6s\n",
      "[CV 1/3; 8/49] START max_depth=8................................................\n",
      "[CV 1/3; 8/49] END ..............................max_depth=8; total time=  10.0s\n",
      "[CV 2/3; 8/49] START max_depth=8................................................\n",
      "[CV 2/3; 8/49] END ..............................max_depth=8; total time=   8.6s\n",
      "[CV 3/3; 8/49] START max_depth=8................................................\n",
      "[CV 3/3; 8/49] END ..............................max_depth=8; total time=   9.6s\n",
      "[CV 1/3; 9/49] START max_depth=9................................................\n",
      "[CV 1/3; 9/49] END ..............................max_depth=9; total time=  10.0s\n",
      "[CV 2/3; 9/49] START max_depth=9................................................\n",
      "[CV 2/3; 9/49] END ..............................max_depth=9; total time=  10.2s\n",
      "[CV 3/3; 9/49] START max_depth=9................................................\n",
      "[CV 3/3; 9/49] END ..............................max_depth=9; total time=  10.0s\n",
      "[CV 1/3; 10/49] START max_depth=10..............................................\n",
      "[CV 1/3; 10/49] END ............................max_depth=10; total time=   9.5s\n",
      "[CV 2/3; 10/49] START max_depth=10..............................................\n",
      "[CV 2/3; 10/49] END ............................max_depth=10; total time=   9.4s\n",
      "[CV 3/3; 10/49] START max_depth=10..............................................\n",
      "[CV 3/3; 10/49] END ............................max_depth=10; total time=   9.4s\n",
      "[CV 1/3; 11/49] START max_depth=11..............................................\n",
      "[CV 1/3; 11/49] END ............................max_depth=11; total time=   9.9s\n",
      "[CV 2/3; 11/49] START max_depth=11..............................................\n",
      "[CV 2/3; 11/49] END ............................max_depth=11; total time=  10.3s\n",
      "[CV 3/3; 11/49] START max_depth=11..............................................\n",
      "[CV 3/3; 11/49] END ............................max_depth=11; total time=   9.9s\n",
      "[CV 1/3; 12/49] START max_depth=12..............................................\n",
      "[CV 1/3; 12/49] END ............................max_depth=12; total time=  12.3s\n",
      "[CV 2/3; 12/49] START max_depth=12..............................................\n",
      "[CV 2/3; 12/49] END ............................max_depth=12; total time=  10.6s\n",
      "[CV 3/3; 12/49] START max_depth=12..............................................\n",
      "[CV 3/3; 12/49] END ............................max_depth=12; total time=  10.2s\n",
      "[CV 1/3; 13/49] START max_depth=13..............................................\n",
      "[CV 1/3; 13/49] END ............................max_depth=13; total time=  11.0s\n",
      "[CV 2/3; 13/49] START max_depth=13..............................................\n",
      "[CV 2/3; 13/49] END ............................max_depth=13; total time=  10.9s\n",
      "[CV 3/3; 13/49] START max_depth=13..............................................\n",
      "[CV 3/3; 13/49] END ............................max_depth=13; total time=  10.9s\n",
      "[CV 1/3; 14/49] START max_depth=14..............................................\n",
      "[CV 1/3; 14/49] END ............................max_depth=14; total time=  11.6s\n",
      "[CV 2/3; 14/49] START max_depth=14..............................................\n",
      "[CV 2/3; 14/49] END ............................max_depth=14; total time=  10.7s\n",
      "[CV 3/3; 14/49] START max_depth=14..............................................\n",
      "[CV 3/3; 14/49] END ............................max_depth=14; total time=  11.6s\n",
      "[CV 1/3; 15/49] START max_depth=15..............................................\n",
      "[CV 1/3; 15/49] END ............................max_depth=15; total time=  11.7s\n",
      "[CV 2/3; 15/49] START max_depth=15..............................................\n",
      "[CV 2/3; 15/49] END ............................max_depth=15; total time=  11.6s\n",
      "[CV 3/3; 15/49] START max_depth=15..............................................\n",
      "[CV 3/3; 15/49] END ............................max_depth=15; total time=  13.1s\n",
      "[CV 1/3; 16/49] START max_depth=16..............................................\n",
      "[CV 1/3; 16/49] END ............................max_depth=16; total time=  11.6s\n",
      "[CV 2/3; 16/49] START max_depth=16..............................................\n",
      "[CV 2/3; 16/49] END ............................max_depth=16; total time=  11.6s\n",
      "[CV 3/3; 16/49] START max_depth=16..............................................\n",
      "[CV 3/3; 16/49] END ............................max_depth=16; total time=  11.9s\n",
      "[CV 1/3; 17/49] START max_depth=17..............................................\n",
      "[CV 1/3; 17/49] END ............................max_depth=17; total time=  12.2s\n",
      "[CV 2/3; 17/49] START max_depth=17..............................................\n",
      "[CV 2/3; 17/49] END ............................max_depth=17; total time=  12.3s\n",
      "[CV 3/3; 17/49] START max_depth=17..............................................\n",
      "[CV 3/3; 17/49] END ............................max_depth=17; total time=  11.8s\n",
      "[CV 1/3; 18/49] START max_depth=18..............................................\n",
      "[CV 1/3; 18/49] END ............................max_depth=18; total time=  12.3s\n",
      "[CV 2/3; 18/49] START max_depth=18..............................................\n",
      "[CV 2/3; 18/49] END ............................max_depth=18; total time=  12.8s\n",
      "[CV 3/3; 18/49] START max_depth=18..............................................\n",
      "[CV 3/3; 18/49] END ............................max_depth=18; total time=  12.4s\n",
      "[CV 1/3; 19/49] START max_depth=19..............................................\n",
      "[CV 1/3; 19/49] END ............................max_depth=19; total time=  13.6s\n",
      "[CV 2/3; 19/49] START max_depth=19..............................................\n",
      "[CV 2/3; 19/49] END ............................max_depth=19; total time=  12.7s\n",
      "[CV 3/3; 19/49] START max_depth=19..............................................\n",
      "[CV 3/3; 19/49] END ............................max_depth=19; total time=  14.0s\n",
      "[CV 1/3; 20/49] START max_depth=20..............................................\n",
      "[CV 1/3; 20/49] END ............................max_depth=20; total time=  13.0s\n",
      "[CV 2/3; 20/49] START max_depth=20..............................................\n",
      "[CV 2/3; 20/49] END ............................max_depth=20; total time=  13.2s\n",
      "[CV 3/3; 20/49] START max_depth=20..............................................\n",
      "[CV 3/3; 20/49] END ............................max_depth=20; total time=  13.0s\n",
      "[CV 1/3; 21/49] START max_depth=21..............................................\n",
      "[CV 1/3; 21/49] END ............................max_depth=21; total time=  13.5s\n",
      "[CV 2/3; 21/49] START max_depth=21..............................................\n",
      "[CV 2/3; 21/49] END ............................max_depth=21; total time=  14.3s\n",
      "[CV 3/3; 21/49] START max_depth=21..............................................\n",
      "[CV 3/3; 21/49] END ............................max_depth=21; total time=  13.5s\n",
      "[CV 1/3; 22/49] START max_depth=22..............................................\n",
      "[CV 1/3; 22/49] END ............................max_depth=22; total time=  14.4s\n",
      "[CV 2/3; 22/49] START max_depth=22..............................................\n",
      "[CV 2/3; 22/49] END ............................max_depth=22; total time=  14.0s\n",
      "[CV 3/3; 22/49] START max_depth=22..............................................\n",
      "[CV 3/3; 22/49] END ............................max_depth=22; total time=  14.4s\n",
      "[CV 1/3; 23/49] START max_depth=23..............................................\n",
      "[CV 1/3; 23/49] END ............................max_depth=23; total time=  14.1s\n",
      "[CV 2/3; 23/49] START max_depth=23..............................................\n",
      "[CV 2/3; 23/49] END ............................max_depth=23; total time=  14.7s\n",
      "[CV 3/3; 23/49] START max_depth=23..............................................\n",
      "[CV 3/3; 23/49] END ............................max_depth=23; total time=  14.3s\n",
      "[CV 1/3; 24/49] START max_depth=24..............................................\n",
      "[CV 1/3; 24/49] END ............................max_depth=24; total time=  15.7s\n",
      "[CV 2/3; 24/49] START max_depth=24..............................................\n",
      "[CV 2/3; 24/49] END ............................max_depth=24; total time=  14.4s\n",
      "[CV 3/3; 24/49] START max_depth=24..............................................\n",
      "[CV 3/3; 24/49] END ............................max_depth=24; total time=  14.5s\n",
      "[CV 1/3; 25/49] START max_depth=25..............................................\n",
      "[CV 1/3; 25/49] END ............................max_depth=25; total time=  14.8s\n",
      "[CV 2/3; 25/49] START max_depth=25..............................................\n",
      "[CV 2/3; 25/49] END ............................max_depth=25; total time=  15.4s\n",
      "[CV 3/3; 25/49] START max_depth=25..............................................\n",
      "[CV 3/3; 25/49] END ............................max_depth=25; total time=  14.7s\n",
      "[CV 1/3; 26/49] START max_depth=26..............................................\n",
      "[CV 1/3; 26/49] END ............................max_depth=26; total time=  15.4s\n",
      "[CV 2/3; 26/49] START max_depth=26..............................................\n",
      "[CV 2/3; 26/49] END ............................max_depth=26; total time=  15.4s\n",
      "[CV 3/3; 26/49] START max_depth=26..............................................\n",
      "[CV 3/3; 26/49] END ............................max_depth=26; total time=  15.9s\n",
      "[CV 1/3; 27/49] START max_depth=27..............................................\n",
      "[CV 1/3; 27/49] END ............................max_depth=27; total time=  16.3s\n",
      "[CV 2/3; 27/49] START max_depth=27..............................................\n",
      "[CV 2/3; 27/49] END ............................max_depth=27; total time=  16.1s\n",
      "[CV 3/3; 27/49] START max_depth=27..............................................\n",
      "[CV 3/3; 27/49] END ............................max_depth=27; total time=  16.6s\n",
      "[CV 1/3; 28/49] START max_depth=28..............................................\n",
      "[CV 1/3; 28/49] END ............................max_depth=28; total time=  16.4s\n",
      "[CV 2/3; 28/49] START max_depth=28..............................................\n",
      "[CV 2/3; 28/49] END ............................max_depth=28; total time=  16.2s\n",
      "[CV 3/3; 28/49] START max_depth=28..............................................\n",
      "[CV 3/3; 28/49] END ............................max_depth=28; total time=  16.1s\n",
      "[CV 1/3; 29/49] START max_depth=29..............................................\n",
      "[CV 1/3; 29/49] END ............................max_depth=29; total time=  16.8s\n",
      "[CV 2/3; 29/49] START max_depth=29..............................................\n",
      "[CV 2/3; 29/49] END ............................max_depth=29; total time=  16.7s\n",
      "[CV 3/3; 29/49] START max_depth=29..............................................\n",
      "[CV 3/3; 29/49] END ............................max_depth=29; total time=  16.6s\n",
      "[CV 1/3; 30/49] START max_depth=30..............................................\n",
      "[CV 1/3; 30/49] END ............................max_depth=30; total time=  16.5s\n",
      "[CV 2/3; 30/49] START max_depth=30..............................................\n",
      "[CV 2/3; 30/49] END ............................max_depth=30; total time=  17.4s\n",
      "[CV 3/3; 30/49] START max_depth=30..............................................\n",
      "[CV 3/3; 30/49] END ............................max_depth=30; total time=  16.9s\n",
      "[CV 1/3; 31/49] START max_depth=31..............................................\n",
      "[CV 1/3; 31/49] END ............................max_depth=31; total time=  16.9s\n",
      "[CV 2/3; 31/49] START max_depth=31..............................................\n",
      "[CV 2/3; 31/49] END ............................max_depth=31; total time=  18.4s\n",
      "[CV 3/3; 31/49] START max_depth=31..............................................\n",
      "[CV 3/3; 31/49] END ............................max_depth=31; total time=  17.1s\n",
      "[CV 1/3; 32/49] START max_depth=32..............................................\n",
      "[CV 1/3; 32/49] END ............................max_depth=32; total time=  17.6s\n",
      "[CV 2/3; 32/49] START max_depth=32..............................................\n",
      "[CV 2/3; 32/49] END ............................max_depth=32; total time=  17.8s\n",
      "[CV 3/3; 32/49] START max_depth=32..............................................\n",
      "[CV 3/3; 32/49] END ............................max_depth=32; total time=  17.4s\n",
      "[CV 1/3; 33/49] START max_depth=33..............................................\n",
      "[CV 1/3; 33/49] END ............................max_depth=33; total time=  17.8s\n",
      "[CV 2/3; 33/49] START max_depth=33..............................................\n",
      "[CV 2/3; 33/49] END ............................max_depth=33; total time=  17.8s\n",
      "[CV 3/3; 33/49] START max_depth=33..............................................\n",
      "[CV 3/3; 33/49] END ............................max_depth=33; total time=  18.0s\n",
      "[CV 1/3; 34/49] START max_depth=34..............................................\n",
      "[CV 1/3; 34/49] END ............................max_depth=34; total time=  18.4s\n",
      "[CV 2/3; 34/49] START max_depth=34..............................................\n",
      "[CV 2/3; 34/49] END ............................max_depth=34; total time=  18.9s\n",
      "[CV 3/3; 34/49] START max_depth=34..............................................\n",
      "[CV 3/3; 34/49] END ............................max_depth=34; total time=  19.0s\n",
      "[CV 1/3; 35/49] START max_depth=35..............................................\n",
      "[CV 1/3; 35/49] END ............................max_depth=35; total time=  19.0s\n",
      "[CV 2/3; 35/49] START max_depth=35..............................................\n",
      "[CV 2/3; 35/49] END ............................max_depth=35; total time=  19.0s\n",
      "[CV 3/3; 35/49] START max_depth=35..............................................\n",
      "[CV 3/3; 35/49] END ............................max_depth=35; total time=  18.1s\n",
      "[CV 1/3; 36/49] START max_depth=36..............................................\n",
      "[CV 1/3; 36/49] END ............................max_depth=36; total time=  19.5s\n",
      "[CV 2/3; 36/49] START max_depth=36..............................................\n",
      "[CV 2/3; 36/49] END ............................max_depth=36; total time=  18.9s\n",
      "[CV 3/3; 36/49] START max_depth=36..............................................\n",
      "[CV 3/3; 36/49] END ............................max_depth=36; total time=  18.1s\n",
      "[CV 1/3; 37/49] START max_depth=37..............................................\n",
      "[CV 1/3; 37/49] END ............................max_depth=37; total time=  19.6s\n",
      "[CV 2/3; 37/49] START max_depth=37..............................................\n",
      "[CV 2/3; 37/49] END ............................max_depth=37; total time=  18.6s\n",
      "[CV 3/3; 37/49] START max_depth=37..............................................\n",
      "[CV 3/3; 37/49] END ............................max_depth=37; total time=  18.6s\n",
      "[CV 1/3; 38/49] START max_depth=38..............................................\n",
      "[CV 1/3; 38/49] END ............................max_depth=38; total time=  20.4s\n",
      "[CV 2/3; 38/49] START max_depth=38..............................................\n",
      "[CV 2/3; 38/49] END ............................max_depth=38; total time=  20.1s\n",
      "[CV 3/3; 38/49] START max_depth=38..............................................\n",
      "[CV 3/3; 38/49] END ............................max_depth=38; total time=  19.5s\n",
      "[CV 1/3; 39/49] START max_depth=39..............................................\n",
      "[CV 1/3; 39/49] END ............................max_depth=39; total time=  20.2s\n",
      "[CV 2/3; 39/49] START max_depth=39..............................................\n",
      "[CV 2/3; 39/49] END ............................max_depth=39; total time=  19.8s\n",
      "[CV 3/3; 39/49] START max_depth=39..............................................\n",
      "[CV 3/3; 39/49] END ............................max_depth=39; total time=  18.9s\n",
      "[CV 1/3; 40/49] START max_depth=40..............................................\n",
      "[CV 1/3; 40/49] END ............................max_depth=40; total time=  19.8s\n",
      "[CV 2/3; 40/49] START max_depth=40..............................................\n",
      "[CV 2/3; 40/49] END ............................max_depth=40; total time=  20.3s\n",
      "[CV 3/3; 40/49] START max_depth=40..............................................\n",
      "[CV 3/3; 40/49] END ............................max_depth=40; total time=  20.0s\n",
      "[CV 1/3; 41/49] START max_depth=41..............................................\n",
      "[CV 1/3; 41/49] END ............................max_depth=41; total time=  21.5s\n",
      "[CV 2/3; 41/49] START max_depth=41..............................................\n",
      "[CV 2/3; 41/49] END ............................max_depth=41; total time=  20.1s\n",
      "[CV 3/3; 41/49] START max_depth=41..............................................\n",
      "[CV 3/3; 41/49] END ............................max_depth=41; total time=  19.6s\n",
      "[CV 1/3; 42/49] START max_depth=42..............................................\n",
      "[CV 1/3; 42/49] END ............................max_depth=42; total time=  21.2s\n",
      "[CV 2/3; 42/49] START max_depth=42..............................................\n",
      "[CV 2/3; 42/49] END ............................max_depth=42; total time=  20.5s\n",
      "[CV 3/3; 42/49] START max_depth=42..............................................\n",
      "[CV 3/3; 42/49] END ............................max_depth=42; total time=  20.0s\n",
      "[CV 1/3; 43/49] START max_depth=43..............................................\n",
      "[CV 1/3; 43/49] END ............................max_depth=43; total time=  21.0s\n",
      "[CV 2/3; 43/49] START max_depth=43..............................................\n",
      "[CV 2/3; 43/49] END ............................max_depth=43; total time=  20.9s\n",
      "[CV 3/3; 43/49] START max_depth=43..............................................\n",
      "[CV 3/3; 43/49] END ............................max_depth=43; total time=  21.3s\n",
      "[CV 1/3; 44/49] START max_depth=44..............................................\n",
      "[CV 1/3; 44/49] END ............................max_depth=44; total time=  21.3s\n",
      "[CV 2/3; 44/49] START max_depth=44..............................................\n",
      "[CV 2/3; 44/49] END ............................max_depth=44; total time=  20.9s\n",
      "[CV 3/3; 44/49] START max_depth=44..............................................\n",
      "[CV 3/3; 44/49] END ............................max_depth=44; total time=  20.5s\n",
      "[CV 1/3; 45/49] START max_depth=45..............................................\n",
      "[CV 1/3; 45/49] END ............................max_depth=45; total time=  21.6s\n",
      "[CV 2/3; 45/49] START max_depth=45..............................................\n",
      "[CV 2/3; 45/49] END ............................max_depth=45; total time=  21.7s\n",
      "[CV 3/3; 45/49] START max_depth=45..............................................\n",
      "[CV 3/3; 45/49] END ............................max_depth=45; total time=  21.2s\n",
      "[CV 1/3; 46/49] START max_depth=46..............................................\n",
      "[CV 1/3; 46/49] END ............................max_depth=46; total time=  22.0s\n",
      "[CV 2/3; 46/49] START max_depth=46..............................................\n",
      "[CV 2/3; 46/49] END ............................max_depth=46; total time=  21.6s\n",
      "[CV 3/3; 46/49] START max_depth=46..............................................\n",
      "[CV 3/3; 46/49] END ............................max_depth=46; total time=  22.7s\n",
      "[CV 1/3; 47/49] START max_depth=47..............................................\n",
      "[CV 1/3; 47/49] END ............................max_depth=47; total time=  22.4s\n",
      "[CV 2/3; 47/49] START max_depth=47..............................................\n",
      "[CV 2/3; 47/49] END ............................max_depth=47; total time=  22.5s\n",
      "[CV 3/3; 47/49] START max_depth=47..............................................\n",
      "[CV 3/3; 47/49] END ............................max_depth=47; total time=  22.7s\n",
      "[CV 1/3; 48/49] START max_depth=48..............................................\n",
      "[CV 1/3; 48/49] END ............................max_depth=48; total time=  23.1s\n",
      "[CV 2/3; 48/49] START max_depth=48..............................................\n",
      "[CV 2/3; 48/49] END ............................max_depth=48; total time=  23.1s\n",
      "[CV 3/3; 48/49] START max_depth=48..............................................\n",
      "[CV 3/3; 48/49] END ............................max_depth=48; total time=  23.1s\n",
      "[CV 1/3; 49/49] START max_depth=49..............................................\n",
      "[CV 1/3; 49/49] END ............................max_depth=49; total time=  23.3s\n",
      "[CV 2/3; 49/49] START max_depth=49..............................................\n",
      "[CV 2/3; 49/49] END ............................max_depth=49; total time=  24.8s\n",
      "[CV 3/3; 49/49] START max_depth=49..............................................\n",
      "[CV 3/3; 49/49] END ............................max_depth=49; total time=  23.2s\n",
      "{'max_depth': 49}\n",
      "CPU times: user 37min 3s, sys: 0 ns, total: 37min 3s\n",
      "Wall time: 37min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'max_depth': range(1, 50, 1)}\n",
    "\n",
    "model_DTC = DecisionTreeClassifier(random_state=12345) \n",
    "\n",
    "model_DTC_gs = GridSearchCV(estimator=model_DTC, \n",
    "                     param_grid=param_grid,\n",
    "                     cv=3, \n",
    "                     n_jobs=-1, \n",
    "                     verbose=10,\n",
    "                     scoring='f1')\n",
    "model_DTC_gs.fit(features_train, target_train)\n",
    "print(model_DTC_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 для DecisionTreeClassifier: 0.7030880057750589\n"
     ]
    }
   ],
   "source": [
    "model_DTC = DecisionTreeClassifier(max_depth=49, random_state=12345)\n",
    "model_DTC.fit(features_train, target_train)\n",
    "scores_DTC = cross_val_score(model_DTC,\n",
    "                            features_train,\n",
    "                            target_train,\n",
    "                            cv=3,\n",
    "                            n_jobs=-1,\n",
    "                            scoring='f1') \n",
    "\n",
    "f1_DTC = scores_DTC.mean()\n",
    "print('f1 для DecisionTreeClassifier:', f1_DTC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат скоринга f1 показала модель LogisticRegression с помощью пайплайн на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, penalty='l1', random_state=12345, solver='liblinear')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774971031286211\n"
     ]
    }
   ],
   "source": [
    "predictions = LR.predict(features_test)\n",
    "print(f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат на тестовой выборке хороший, 0.78. Получилось обучить модель классифицировать комментарии на позитивные и негативные. \n",
    "В работе сначала были подготовленны данные: проведена леммаизация признака, выборку разделила на обучающую и тестовую, убрала стоп слова и перевела текст в векторы  методом TfidfVectorizer.\n",
    "Затем были протестирование 3 вида моделей, из них выбрана лучшая: LogisticRegression. Лусщую модель протестироваа на тестовой выборке. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3237,
    "start_time": "2023-04-13T13:01:25.289Z"
   },
   {
    "duration": 2660,
    "start_time": "2023-04-13T13:01:28.528Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-13T13:01:31.190Z"
   },
   {
    "duration": 149,
    "start_time": "2023-04-13T13:10:09.462Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-13T13:10:19.047Z"
   },
   {
    "duration": 374,
    "start_time": "2023-04-13T13:13:57.627Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-13T13:42:48.810Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-13T13:42:50.304Z"
   },
   {
    "duration": 20,
    "start_time": "2023-04-13T13:42:52.280Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-13T13:43:20.193Z"
   },
   {
    "duration": 45,
    "start_time": "2023-04-13T13:43:23.988Z"
   },
   {
    "duration": 106,
    "start_time": "2023-04-13T13:44:13.396Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-13T13:44:13.505Z"
   },
   {
    "duration": 3294,
    "start_time": "2023-04-13T13:44:25.601Z"
   },
   {
    "duration": 4590,
    "start_time": "2023-04-13T13:44:30.507Z"
   },
   {
    "duration": 18,
    "start_time": "2023-04-13T13:44:36.517Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-13T13:44:38.794Z"
   },
   {
    "duration": 335,
    "start_time": "2023-04-13T13:44:40.688Z"
   },
   {
    "duration": 5543,
    "start_time": "2023-04-13T13:45:03.685Z"
   },
   {
    "duration": 3602,
    "start_time": "2023-04-13T13:45:09.230Z"
   },
   {
    "duration": 29,
    "start_time": "2023-04-13T13:45:12.834Z"
   },
   {
    "duration": 25,
    "start_time": "2023-04-13T13:45:12.879Z"
   },
   {
    "duration": 521,
    "start_time": "2023-04-13T13:45:12.905Z"
   },
   {
    "duration": 5629,
    "start_time": "2023-04-13T13:45:28.292Z"
   },
   {
    "duration": 3677,
    "start_time": "2023-04-13T13:45:33.923Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-13T13:45:37.602Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-13T13:45:37.618Z"
   },
   {
    "duration": 454,
    "start_time": "2023-04-13T13:45:37.632Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-13T13:45:38.088Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-13T13:45:38.092Z"
   },
   {
    "duration": 229,
    "start_time": "2023-04-13T13:45:41.776Z"
   },
   {
    "duration": 37,
    "start_time": "2023-04-13T13:46:49.834Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-13T13:49:25.931Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-13T14:10:41.237Z"
   },
   {
    "duration": 8009,
    "start_time": "2023-04-13T14:15:51.283Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-13T14:17:03.484Z"
   },
   {
    "duration": 3473,
    "start_time": "2023-04-13T14:17:03.718Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-13T14:17:07.193Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-13T14:17:07.203Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-13T14:17:07.221Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-13T14:17:07.228Z"
   },
   {
    "duration": 6862,
    "start_time": "2023-04-13T14:17:11.474Z"
   },
   {
    "duration": 19,
    "start_time": "2023-04-13T14:17:26.955Z"
   },
   {
    "duration": 17,
    "start_time": "2023-04-13T14:19:06.676Z"
   },
   {
    "duration": 1563,
    "start_time": "2023-04-13T14:19:06.885Z"
   },
   {
    "duration": 16,
    "start_time": "2023-04-13T14:19:08.463Z"
   },
   {
    "duration": 16,
    "start_time": "2023-04-13T14:19:08.480Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-13T14:19:08.498Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-13T14:19:08.506Z"
   },
   {
    "duration": 6827,
    "start_time": "2023-04-13T14:19:09.679Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-13T14:19:16.508Z"
   },
   {
    "duration": 2491,
    "start_time": "2023-04-13T14:19:38.517Z"
   },
   {
    "duration": 1061,
    "start_time": "2023-04-13T14:19:41.010Z"
   },
   {
    "duration": 37,
    "start_time": "2023-04-13T14:19:42.072Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-13T14:19:42.111Z"
   },
   {
    "duration": 378,
    "start_time": "2023-04-13T14:19:42.122Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-13T14:19:42.501Z"
   },
   {
    "duration": 180,
    "start_time": "2023-04-13T14:19:42.505Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-13T14:19:42.692Z"
   },
   {
    "duration": 1742,
    "start_time": "2023-04-13T14:20:06.737Z"
   },
   {
    "duration": 3689,
    "start_time": "2023-04-13T14:25:02.299Z"
   },
   {
    "duration": 1051,
    "start_time": "2023-04-13T14:25:08.804Z"
   },
   {
    "duration": 16,
    "start_time": "2023-04-13T14:25:10.475Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-13T14:25:11.504Z"
   },
   {
    "duration": 394,
    "start_time": "2023-04-13T14:25:12.737Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-13T14:25:13.934Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-13T14:27:54.296Z"
   },
   {
    "duration": 946,
    "start_time": "2023-04-13T14:27:54.300Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-13T14:27:55.256Z"
   },
   {
    "duration": 36,
    "start_time": "2023-04-13T14:27:55.268Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-13T14:27:55.305Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-13T14:27:55.313Z"
   },
   {
    "duration": 1503,
    "start_time": "2023-04-13T14:29:50.838Z"
   },
   {
    "duration": 1187,
    "start_time": "2023-04-13T14:29:53.336Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-13T14:29:54.526Z"
   },
   {
    "duration": 41,
    "start_time": "2023-04-13T14:29:54.555Z"
   },
   {
    "duration": 232,
    "start_time": "2023-04-13T14:29:54.598Z"
   },
   {
    "duration": 2,
    "start_time": "2023-04-13T14:29:54.832Z"
   },
   {
    "duration": 1317,
    "start_time": "2023-04-13T14:29:55.525Z"
   },
   {
    "duration": 1700397,
    "start_time": "2023-04-13T14:30:23.811Z"
   },
   {
    "duration": 20,
    "start_time": "2023-04-13T15:46:01.377Z"
   },
   {
    "duration": 56,
    "start_time": "2023-04-14T08:03:14.894Z"
   },
   {
    "duration": 1496,
    "start_time": "2023-04-14T08:03:21.308Z"
   },
   {
    "duration": 3023,
    "start_time": "2023-04-14T08:03:23.655Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-14T08:03:27.330Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-14T08:03:29.583Z"
   },
   {
    "duration": 478,
    "start_time": "2023-04-14T08:03:30.443Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-14T08:03:32.589Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-14T08:03:38.253Z"
   },
   {
    "duration": 1328767,
    "start_time": "2023-04-14T08:03:46.780Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-14T12:14:34.528Z"
   },
   {
    "duration": 82,
    "start_time": "2023-04-14T12:14:40.214Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-14T12:15:31.303Z"
   },
   {
    "duration": 68,
    "start_time": "2023-04-14T12:15:36.517Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-14T12:16:01.665Z"
   },
   {
    "duration": 143,
    "start_time": "2023-04-14T12:16:09.545Z"
   },
   {
    "duration": 7144,
    "start_time": "2023-04-14T12:16:12.079Z"
   },
   {
    "duration": 38,
    "start_time": "2023-04-14T12:16:21.503Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-14T12:18:08.003Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-14T12:18:30.231Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-14T12:19:14.404Z"
   },
   {
    "duration": 25,
    "start_time": "2023-04-14T12:33:23.754Z"
   },
   {
    "duration": 21,
    "start_time": "2023-04-14T12:33:40.689Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-14T12:34:48.762Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-14T12:35:27.280Z"
   },
   {
    "duration": 916,
    "start_time": "2023-04-14T12:35:27.364Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-14T12:35:28.282Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-14T12:35:28.293Z"
   },
   {
    "duration": 26,
    "start_time": "2023-04-14T12:35:28.309Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-14T12:35:28.755Z"
   },
   {
    "duration": 1366675,
    "start_time": "2023-04-14T12:35:29.152Z"
   },
   {
    "duration": 69,
    "start_time": "2023-04-14T12:58:15.829Z"
   },
   {
    "duration": 126,
    "start_time": "2023-04-14T12:58:15.900Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-14T12:58:16.028Z"
   },
   {
    "duration": 57,
    "start_time": "2023-04-14T12:58:16.033Z"
   },
   {
    "duration": 8039,
    "start_time": "2023-04-14T12:58:16.092Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-14T13:01:22.595Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-14T13:01:34.608Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-14T13:12:35.151Z"
   },
   {
    "duration": 44,
    "start_time": "2023-04-17T09:08:34.700Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-17T09:08:55.129Z"
   },
   {
    "duration": 1280,
    "start_time": "2023-04-17T09:09:03.465Z"
   },
   {
    "duration": 2687,
    "start_time": "2023-04-17T09:09:04.747Z"
   },
   {
    "duration": 13,
    "start_time": "2023-04-17T09:09:07.435Z"
   },
   {
    "duration": 13,
    "start_time": "2023-04-17T09:09:07.451Z"
   },
   {
    "duration": 274,
    "start_time": "2023-04-17T09:09:07.466Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T09:09:07.742Z"
   },
   {
    "duration": 971647,
    "start_time": "2023-04-17T09:09:07.746Z"
   },
   {
    "duration": 13,
    "start_time": "2023-04-17T09:25:19.395Z"
   },
   {
    "duration": 167,
    "start_time": "2023-04-17T09:25:19.409Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T09:25:19.578Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-17T09:25:19.582Z"
   },
   {
    "duration": 7311,
    "start_time": "2023-04-17T09:25:19.597Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T09:25:26.909Z"
   },
   {
    "duration": 28,
    "start_time": "2023-04-17T09:25:26.913Z"
   },
   {
    "duration": 416733,
    "start_time": "2023-04-17T09:25:26.942Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T09:32:23.677Z"
   },
   {
    "duration": 41850,
    "start_time": "2023-04-17T10:12:56.889Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-17T10:30:17.147Z"
   },
   {
    "duration": 159079,
    "start_time": "2023-04-17T10:30:22.018Z"
   },
   {
    "duration": 24,
    "start_time": "2023-04-17T10:37:24.364Z"
   },
   {
    "duration": 220,
    "start_time": "2023-04-17T10:37:29.920Z"
   },
   {
    "duration": 385,
    "start_time": "2023-04-17T10:38:12.228Z"
   },
   {
    "duration": 4876,
    "start_time": "2023-04-17T10:39:05.910Z"
   },
   {
    "duration": 485952,
    "start_time": "2023-04-17T10:39:27.033Z"
   },
   {
    "duration": 868,
    "start_time": "2023-04-17T11:07:38.212Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T11:17:47.178Z"
   },
   {
    "duration": 2241818,
    "start_time": "2023-04-17T11:17:52.071Z"
   },
   {
    "duration": 98694,
    "start_time": "2023-04-17T12:03:40.067Z"
   },
   {
    "duration": 100257,
    "start_time": "2023-04-17T12:05:58.960Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-17T12:08:09.305Z"
   },
   {
    "duration": 73,
    "start_time": "2023-04-17T12:08:41.859Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T12:08:49.027Z"
   },
   {
    "duration": 195,
    "start_time": "2023-04-17T12:08:55.144Z"
   },
   {
    "duration": 326,
    "start_time": "2023-04-17T12:10:46.825Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T12:10:55.040Z"
   },
   {
    "duration": 221,
    "start_time": "2023-04-17T12:10:59.859Z"
   },
   {
    "duration": 169,
    "start_time": "2023-04-17T12:11:56.436Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-17T12:12:02.775Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-17T12:12:10.564Z"
   },
   {
    "duration": 1341,
    "start_time": "2023-04-18T11:55:20.951Z"
   },
   {
    "duration": 3459,
    "start_time": "2023-04-18T11:55:22.294Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-18T11:55:25.754Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-18T11:55:25.770Z"
   },
   {
    "duration": 312,
    "start_time": "2023-04-18T11:55:25.778Z"
   },
   {
    "duration": 2,
    "start_time": "2023-04-18T11:55:26.092Z"
   },
   {
    "duration": 1168960,
    "start_time": "2023-04-18T11:55:26.096Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-18T12:35:38.827Z"
   },
   {
    "duration": 878,
    "start_time": "2023-04-18T12:35:54.223Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-18T12:35:55.710Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-18T12:35:56.466Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-18T12:35:57.220Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-18T12:35:58.029Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-18T12:35:58.314Z"
   },
   {
    "duration": 346,
    "start_time": "2023-04-18T12:36:04.580Z"
   },
   {
    "duration": 229,
    "start_time": "2023-04-18T12:36:52.775Z"
   },
   {
    "duration": 213,
    "start_time": "2023-04-18T12:37:25.779Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-18T12:43:58.686Z"
   },
   {
    "duration": 813,
    "start_time": "2023-04-18T12:44:07.532Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-18T12:44:08.347Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-18T12:44:08.478Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-18T12:44:08.826Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-18T12:44:09.923Z"
   },
   {
    "duration": 13,
    "start_time": "2023-04-18T12:44:10.823Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-18T12:45:10.335Z"
   },
   {
    "duration": 231,
    "start_time": "2023-04-18T12:45:15.668Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-18T12:49:30.933Z"
   },
   {
    "duration": 293,
    "start_time": "2023-04-18T12:49:33.865Z"
   },
   {
    "duration": 282,
    "start_time": "2023-04-18T12:49:47.738Z"
   },
   {
    "duration": 241,
    "start_time": "2023-04-18T12:57:48.258Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-18T12:58:18.122Z"
   },
   {
    "duration": 195,
    "start_time": "2023-04-18T12:58:18.758Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-18T12:59:01.322Z"
   },
   {
    "duration": 240,
    "start_time": "2023-04-18T12:59:01.857Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-18T13:02:18.264Z"
   },
   {
    "duration": 208,
    "start_time": "2023-04-18T13:02:18.456Z"
   },
   {
    "duration": 1271,
    "start_time": "2023-04-18T13:03:41.141Z"
   },
   {
    "duration": 922,
    "start_time": "2023-04-18T13:03:43.474Z"
   },
   {
    "duration": 21,
    "start_time": "2023-04-18T13:03:44.398Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-18T13:03:44.422Z"
   },
   {
    "duration": 175,
    "start_time": "2023-04-18T13:03:44.433Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-18T13:03:52.729Z"
   },
   {
    "duration": 326,
    "start_time": "2023-04-18T13:03:54.244Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-18T13:11:13.407Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-18T13:16:07.752Z"
   },
   {
    "duration": 308,
    "start_time": "2023-04-18T13:16:08.802Z"
   },
   {
    "duration": 10535,
    "start_time": "2023-04-19T05:28:57.430Z"
   },
   {
    "duration": 7093,
    "start_time": "2023-04-19T05:29:07.967Z"
   },
   {
    "duration": 27,
    "start_time": "2023-04-19T05:29:15.067Z"
   },
   {
    "duration": 149,
    "start_time": "2023-04-19T05:29:15.100Z"
   },
   {
    "duration": 654,
    "start_time": "2023-04-19T05:29:15.256Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-19T05:29:15.913Z"
   },
   {
    "duration": 1183,
    "start_time": "2023-04-19T05:29:15.926Z"
   },
   {
    "duration": 1808,
    "start_time": "2023-04-19T05:29:31.605Z"
   },
   {
    "duration": 6991,
    "start_time": "2023-04-19T05:29:35.637Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-19T05:29:43.671Z"
   },
   {
    "duration": 1436,
    "start_time": "2023-04-19T05:40:34.794Z"
   },
   {
    "duration": 2737,
    "start_time": "2023-04-19T05:40:38.445Z"
   },
   {
    "duration": 17,
    "start_time": "2023-04-19T05:40:41.184Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-19T05:40:41.202Z"
   },
   {
    "duration": 315,
    "start_time": "2023-04-19T05:40:41.215Z"
   },
   {
    "duration": 449,
    "start_time": "2023-04-19T05:40:45.591Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T05:40:47.687Z"
   },
   {
    "duration": 1191424,
    "start_time": "2023-04-19T05:40:48.422Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-19T06:00:39.848Z"
   },
   {
    "duration": 91,
    "start_time": "2023-04-19T07:03:01.827Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-19T07:03:02.746Z"
   },
   {
    "duration": 200,
    "start_time": "2023-04-19T07:03:05.112Z"
   },
   {
    "duration": 7401,
    "start_time": "2023-04-19T07:03:05.659Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-19T07:03:15.766Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-19T07:03:48.691Z"
   },
   {
    "duration": 134,
    "start_time": "2023-04-19T07:03:54.919Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-19T07:04:47.130Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-19T07:04:53.227Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-19T07:24:31.694Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-19T07:25:28.234Z"
   },
   {
    "duration": 58,
    "start_time": "2023-04-19T07:25:31.420Z"
   },
   {
    "duration": 53,
    "start_time": "2023-04-19T07:31:14.773Z"
   },
   {
    "duration": 195,
    "start_time": "2023-04-19T07:37:16.036Z"
   },
   {
    "duration": 24,
    "start_time": "2023-04-19T07:38:18.698Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-19T07:38:31.470Z"
   },
   {
    "duration": 888,
    "start_time": "2023-04-19T07:38:32.617Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-19T07:38:33.507Z"
   },
   {
    "duration": 20,
    "start_time": "2023-04-19T07:38:33.517Z"
   },
   {
    "duration": 29,
    "start_time": "2023-04-19T07:38:33.796Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-19T07:38:34.733Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-19T07:38:34.922Z"
   },
   {
    "duration": 1236727,
    "start_time": "2023-04-19T07:38:35.112Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-19T07:59:11.841Z"
   },
   {
    "duration": 40,
    "start_time": "2023-04-19T07:59:11.854Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-19T07:59:11.896Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-19T07:59:11.896Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-19T07:59:11.898Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-19T07:59:11.899Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-19T07:59:11.900Z"
   },
   {
    "duration": 59,
    "start_time": "2023-04-19T08:28:56.098Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T08:28:57.123Z"
   },
   {
    "duration": 207,
    "start_time": "2023-04-19T08:28:57.347Z"
   },
   {
    "duration": 6736,
    "start_time": "2023-04-19T08:28:57.561Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T08:29:04.299Z"
   },
   {
    "duration": 17,
    "start_time": "2023-04-19T08:29:04.304Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-19T08:29:04.323Z"
   },
   {
    "duration": 517,
    "start_time": "2023-04-19T08:29:04.336Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T08:31:07.600Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-19T08:31:15.852Z"
   },
   {
    "duration": 534,
    "start_time": "2023-04-19T08:32:38.529Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-19T08:48:37.251Z"
   },
   {
    "duration": 27,
    "start_time": "2023-04-19T08:48:48.763Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T09:03:15.358Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-19T09:04:02.541Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-19T09:04:14.112Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T09:04:14.294Z"
   },
   {
    "duration": 28,
    "start_time": "2023-04-19T09:04:14.463Z"
   },
   {
    "duration": 33,
    "start_time": "2023-04-19T09:04:38.768Z"
   },
   {
    "duration": 73,
    "start_time": "2023-04-19T09:04:43.657Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T09:04:44.495Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-19T09:04:45.071Z"
   },
   {
    "duration": 6338,
    "start_time": "2023-04-19T09:04:46.516Z"
   },
   {
    "duration": 2,
    "start_time": "2023-04-19T09:04:52.856Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-19T09:05:35.487Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-19T09:05:38.671Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-19T09:05:38.989Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-19T09:05:39.999Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T09:06:13.394Z"
   },
   {
    "duration": 524,
    "start_time": "2023-04-19T09:06:14.036Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T09:06:17.732Z"
   },
   {
    "duration": 3714,
    "start_time": "2023-04-19T09:09:09.498Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-19T09:10:09.897Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-19T09:10:17.517Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
